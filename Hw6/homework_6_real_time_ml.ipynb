{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\legon\\miniconda3\\envs\\RealTimeenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndevNumber = torch.cuda.current_device()\\ndevName = torch.cuda.get_device_name(devNumber)\\n\\nprint(f\"Current device number is: {devNumber}\")\\nprint(f\"GPU name is: {devName}\")'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import requests\n",
    "\n",
    "#including runtime measurements, accuracy metrics, and model size calculations (Homework 6)\n",
    "#For training the models with different layers and heads\n",
    "from itertools import product\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "import math\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "\n",
    "#Importing the Swin Transformer model from Hugging Face Transformers library for Problem 3\n",
    "import transformers \n",
    "from transformers import SwinForImageClassification, SwinConfig, AutoImageProcessor\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "#Check the GPU name and number\n",
    "'''\n",
    "devNumber = torch.cuda.current_device()\n",
    "devName = torch.cuda.get_device_name(devNumber)\n",
    "\n",
    "print(f\"Current device number is: {devNumber}\")\n",
    "print(f\"GPU name is: {devName}\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training ViT-Tiny configuration...\n",
      "Patch size: 4, Embed dim: 256, Depth: 4, Heads: 2, MLP ratio: 2\n",
      "\n",
      "Model Summary:\n",
      "Epoch [1/20], Step [100/782], Loss: 4.1893\n",
      "Epoch [1/20], Step [200/782], Loss: 3.8480\n",
      "Epoch [1/20], Step [300/782], Loss: 4.0729\n",
      "Epoch [1/20], Step [400/782], Loss: 3.8686\n",
      "Epoch [1/20], Step [500/782], Loss: 3.8387\n",
      "Epoch [1/20], Step [600/782], Loss: 3.9619\n",
      "Epoch [1/20], Step [700/782], Loss: 3.6748\n",
      "Epoch 1 completed in 14.78 seconds\n",
      "Epoch [2/20], Step [100/782], Loss: 3.7500\n",
      "Epoch [2/20], Step [200/782], Loss: 3.6387\n",
      "Epoch [2/20], Step [300/782], Loss: 3.5436\n",
      "Epoch [2/20], Step [400/782], Loss: 3.8163\n",
      "Epoch [2/20], Step [500/782], Loss: 3.4327\n",
      "Epoch [2/20], Step [600/782], Loss: 3.4373\n",
      "Epoch [2/20], Step [700/782], Loss: 3.6836\n",
      "Epoch 2 completed in 14.90 seconds\n",
      "Epoch [3/20], Step [100/782], Loss: 3.8262\n",
      "Epoch [3/20], Step [200/782], Loss: 3.2702\n",
      "Epoch [3/20], Step [300/782], Loss: 3.3842\n",
      "Epoch [3/20], Step [400/782], Loss: 3.6353\n",
      "Epoch [3/20], Step [500/782], Loss: 3.3086\n",
      "Epoch [3/20], Step [600/782], Loss: 3.6923\n",
      "Epoch [3/20], Step [700/782], Loss: 3.6756\n",
      "Epoch 3 completed in 15.70 seconds\n",
      "Epoch [4/20], Step [100/782], Loss: 3.4707\n",
      "Epoch [4/20], Step [200/782], Loss: 3.3015\n",
      "Epoch [4/20], Step [300/782], Loss: 3.5264\n",
      "Epoch [4/20], Step [400/782], Loss: 3.4879\n",
      "Epoch [4/20], Step [500/782], Loss: 3.4729\n",
      "Epoch [4/20], Step [600/782], Loss: 3.3020\n",
      "Epoch [4/20], Step [700/782], Loss: 3.5053\n",
      "Epoch 4 completed in 14.83 seconds\n",
      "Epoch [5/20], Step [100/782], Loss: 3.3371\n",
      "Epoch [5/20], Step [200/782], Loss: 3.1652\n",
      "Epoch [5/20], Step [300/782], Loss: 3.1740\n",
      "Epoch [5/20], Step [400/782], Loss: 3.3836\n",
      "Epoch [5/20], Step [500/782], Loss: 3.4710\n",
      "Epoch [5/20], Step [600/782], Loss: 3.4689\n",
      "Epoch [5/20], Step [700/782], Loss: 3.3070\n",
      "Epoch 5 completed in 14.88 seconds\n",
      "Epoch [6/20], Step [100/782], Loss: 3.5617\n",
      "Epoch [6/20], Step [200/782], Loss: 3.4571\n",
      "Epoch [6/20], Step [300/782], Loss: 3.4291\n",
      "Epoch [6/20], Step [400/782], Loss: 3.2626\n",
      "Epoch [6/20], Step [500/782], Loss: 3.2406\n",
      "Epoch [6/20], Step [600/782], Loss: 3.0989\n",
      "Epoch [6/20], Step [700/782], Loss: 3.4954\n",
      "Epoch 6 completed in 14.70 seconds\n",
      "Epoch [7/20], Step [100/782], Loss: 3.1537\n",
      "Epoch [7/20], Step [200/782], Loss: 3.2061\n",
      "Epoch [7/20], Step [300/782], Loss: 3.5686\n",
      "Epoch [7/20], Step [400/782], Loss: 3.3334\n",
      "Epoch [7/20], Step [500/782], Loss: 3.4754\n",
      "Epoch [7/20], Step [600/782], Loss: 3.0808\n",
      "Epoch [7/20], Step [700/782], Loss: 3.2202\n",
      "Epoch 7 completed in 14.92 seconds\n",
      "Epoch [8/20], Step [100/782], Loss: 3.0981\n",
      "Epoch [8/20], Step [200/782], Loss: 3.2587\n",
      "Epoch [8/20], Step [300/782], Loss: 3.3766\n",
      "Epoch [8/20], Step [400/782], Loss: 2.8593\n",
      "Epoch [8/20], Step [500/782], Loss: 3.1203\n",
      "Epoch [8/20], Step [600/782], Loss: 3.3005\n",
      "Epoch [8/20], Step [700/782], Loss: 2.9931\n",
      "Epoch 8 completed in 15.22 seconds\n",
      "Epoch [9/20], Step [100/782], Loss: 3.1013\n",
      "Epoch [9/20], Step [200/782], Loss: 3.4307\n",
      "Epoch [9/20], Step [300/782], Loss: 3.4366\n",
      "Epoch [9/20], Step [400/782], Loss: 3.0823\n",
      "Epoch [9/20], Step [500/782], Loss: 3.2046\n",
      "Epoch [9/20], Step [600/782], Loss: 3.3585\n",
      "Epoch [9/20], Step [700/782], Loss: 3.3207\n",
      "Epoch 9 completed in 15.32 seconds\n",
      "Epoch [10/20], Step [100/782], Loss: 3.0101\n",
      "Epoch [10/20], Step [200/782], Loss: 3.3386\n",
      "Epoch [10/20], Step [300/782], Loss: 3.4305\n",
      "Epoch [10/20], Step [400/782], Loss: 3.3003\n",
      "Epoch [10/20], Step [500/782], Loss: 2.8272\n",
      "Epoch [10/20], Step [600/782], Loss: 3.1549\n",
      "Epoch [10/20], Step [700/782], Loss: 3.3244\n",
      "Epoch 10 completed in 15.54 seconds\n",
      "Epoch [11/20], Step [100/782], Loss: 2.9413\n",
      "Epoch [11/20], Step [200/782], Loss: 3.4670\n",
      "Epoch [11/20], Step [300/782], Loss: 3.1111\n",
      "Epoch [11/20], Step [400/782], Loss: 3.2034\n",
      "Epoch [11/20], Step [500/782], Loss: 3.2946\n",
      "Epoch [11/20], Step [600/782], Loss: 3.3191\n",
      "Epoch [11/20], Step [700/782], Loss: 3.3279\n",
      "Epoch 11 completed in 16.04 seconds\n",
      "Epoch [12/20], Step [100/782], Loss: 3.2213\n",
      "Epoch [12/20], Step [200/782], Loss: 3.2310\n",
      "Epoch [12/20], Step [300/782], Loss: 3.1321\n",
      "Epoch [12/20], Step [400/782], Loss: 3.1122\n",
      "Epoch [12/20], Step [500/782], Loss: 3.4356\n",
      "Epoch [12/20], Step [600/782], Loss: 3.4112\n",
      "Epoch [12/20], Step [700/782], Loss: 3.3506\n",
      "Epoch 12 completed in 15.63 seconds\n",
      "Epoch [13/20], Step [100/782], Loss: 3.0277\n",
      "Epoch [13/20], Step [200/782], Loss: 3.3256\n",
      "Epoch [13/20], Step [300/782], Loss: 3.3187\n",
      "Epoch [13/20], Step [400/782], Loss: 3.1221\n",
      "Epoch [13/20], Step [500/782], Loss: 3.0187\n",
      "Epoch [13/20], Step [600/782], Loss: 3.2980\n",
      "Epoch [13/20], Step [700/782], Loss: 2.8565\n",
      "Epoch 13 completed in 15.50 seconds\n",
      "Epoch [14/20], Step [100/782], Loss: 3.0922\n",
      "Epoch [14/20], Step [200/782], Loss: 3.0091\n",
      "Epoch [14/20], Step [300/782], Loss: 2.8854\n",
      "Epoch [14/20], Step [400/782], Loss: 3.2286\n",
      "Epoch [14/20], Step [500/782], Loss: 3.1673\n",
      "Epoch [14/20], Step [600/782], Loss: 3.1623\n",
      "Epoch [14/20], Step [700/782], Loss: 3.4068\n",
      "Epoch 14 completed in 15.27 seconds\n",
      "Epoch [15/20], Step [100/782], Loss: 2.9468\n",
      "Epoch [15/20], Step [200/782], Loss: 3.0226\n",
      "Epoch [15/20], Step [300/782], Loss: 2.8985\n",
      "Epoch [15/20], Step [400/782], Loss: 3.0866\n",
      "Epoch [15/20], Step [500/782], Loss: 2.8679\n",
      "Epoch [15/20], Step [600/782], Loss: 3.2577\n",
      "Epoch [15/20], Step [700/782], Loss: 3.0931\n",
      "Epoch 15 completed in 15.76 seconds\n",
      "Epoch [16/20], Step [100/782], Loss: 2.9675\n",
      "Epoch [16/20], Step [200/782], Loss: 3.1289\n",
      "Epoch [16/20], Step [300/782], Loss: 2.9190\n",
      "Epoch [16/20], Step [400/782], Loss: 3.2757\n",
      "Epoch [16/20], Step [500/782], Loss: 2.6945\n",
      "Epoch [16/20], Step [600/782], Loss: 2.9967\n",
      "Epoch [16/20], Step [700/782], Loss: 2.7713\n",
      "Epoch 16 completed in 15.41 seconds\n",
      "Epoch [17/20], Step [100/782], Loss: 3.1066\n",
      "Epoch [17/20], Step [200/782], Loss: 3.4125\n",
      "Epoch [17/20], Step [300/782], Loss: 2.9924\n",
      "Epoch [17/20], Step [400/782], Loss: 3.0730\n",
      "Epoch [17/20], Step [500/782], Loss: 3.3037\n",
      "Epoch [17/20], Step [600/782], Loss: 3.0907\n",
      "Epoch [17/20], Step [700/782], Loss: 3.3994\n",
      "Epoch 17 completed in 15.29 seconds\n",
      "Epoch [18/20], Step [100/782], Loss: 3.3419\n",
      "Epoch [18/20], Step [200/782], Loss: 3.3789\n",
      "Epoch [18/20], Step [300/782], Loss: 3.5301\n",
      "Epoch [18/20], Step [400/782], Loss: 3.9658\n",
      "Epoch [18/20], Step [500/782], Loss: 3.5864\n",
      "Epoch [18/20], Step [600/782], Loss: 3.3884\n",
      "Epoch [18/20], Step [700/782], Loss: 3.3058\n",
      "Epoch 18 completed in 15.24 seconds\n",
      "Epoch [19/20], Step [100/782], Loss: 3.2119\n",
      "Epoch [19/20], Step [200/782], Loss: 3.2620\n",
      "Epoch [19/20], Step [300/782], Loss: 3.4329\n",
      "Epoch [19/20], Step [400/782], Loss: 3.2060\n",
      "Epoch [19/20], Step [500/782], Loss: 3.1326\n",
      "Epoch [19/20], Step [600/782], Loss: 3.4437\n",
      "Epoch [19/20], Step [700/782], Loss: 3.0860\n",
      "Epoch 19 completed in 15.26 seconds\n",
      "Epoch [20/20], Step [100/782], Loss: 3.7428\n",
      "Epoch [20/20], Step [200/782], Loss: 3.4917\n",
      "Epoch [20/20], Step [300/782], Loss: 3.3617\n",
      "Epoch [20/20], Step [400/782], Loss: 3.2143\n",
      "Epoch [20/20], Step [500/782], Loss: 2.8895\n",
      "Epoch [20/20], Step [600/782], Loss: 3.2156\n",
      "Epoch [20/20], Step [700/782], Loss: 3.0104\n",
      "Epoch 20 completed in 15.45 seconds\n",
      "Test Accuracy: 18.58%\n",
      "\n",
      "Training ViT-Small configuration...\n",
      "Patch size: 8, Embed dim: 256, Depth: 8, Heads: 2, MLP ratio: 2\n",
      "\n",
      "Model Summary:\n",
      "Epoch [1/20], Step [100/782], Loss: 4.3689\n",
      "Epoch [1/20], Step [200/782], Loss: 4.3427\n",
      "Epoch [1/20], Step [300/782], Loss: 4.4209\n",
      "Epoch [1/20], Step [400/782], Loss: 4.0977\n",
      "Epoch [1/20], Step [500/782], Loss: 4.2614\n",
      "Epoch [1/20], Step [600/782], Loss: 4.1782\n",
      "Epoch [1/20], Step [700/782], Loss: 4.1145\n",
      "Epoch 1 completed in 21.94 seconds\n",
      "Epoch [2/20], Step [100/782], Loss: 4.2227\n",
      "Epoch [2/20], Step [200/782], Loss: 4.1578\n",
      "Epoch [2/20], Step [300/782], Loss: 4.1254\n",
      "Epoch [2/20], Step [400/782], Loss: 4.0313\n",
      "Epoch [2/20], Step [500/782], Loss: 3.9452\n",
      "Epoch [2/20], Step [600/782], Loss: 4.0279\n",
      "Epoch [2/20], Step [700/782], Loss: 4.1391\n",
      "Epoch 2 completed in 21.86 seconds\n",
      "Epoch [3/20], Step [100/782], Loss: 4.0505\n",
      "Epoch [3/20], Step [200/782], Loss: 4.2261\n",
      "Epoch [3/20], Step [300/782], Loss: 3.9395\n",
      "Epoch [3/20], Step [400/782], Loss: 4.3512\n",
      "Epoch [3/20], Step [500/782], Loss: 4.0382\n",
      "Epoch [3/20], Step [600/782], Loss: 3.7168\n",
      "Epoch [3/20], Step [700/782], Loss: 4.1881\n",
      "Epoch 3 completed in 21.69 seconds\n",
      "Epoch [4/20], Step [100/782], Loss: 3.9072\n",
      "Epoch [4/20], Step [200/782], Loss: 3.9793\n",
      "Epoch [4/20], Step [300/782], Loss: 4.2927\n",
      "Epoch [4/20], Step [400/782], Loss: 4.0248\n",
      "Epoch [4/20], Step [500/782], Loss: 4.1590\n",
      "Epoch [4/20], Step [600/782], Loss: 4.0861\n",
      "Epoch [4/20], Step [700/782], Loss: 4.2147\n",
      "Epoch 4 completed in 21.97 seconds\n",
      "Epoch [5/20], Step [100/782], Loss: 3.9940\n",
      "Epoch [5/20], Step [200/782], Loss: 3.7701\n",
      "Epoch [5/20], Step [300/782], Loss: 4.1494\n",
      "Epoch [5/20], Step [400/782], Loss: 4.1968\n",
      "Epoch [5/20], Step [500/782], Loss: 3.9785\n",
      "Epoch [5/20], Step [600/782], Loss: 4.0958\n",
      "Epoch [5/20], Step [700/782], Loss: 4.0135\n",
      "Epoch 5 completed in 21.91 seconds\n",
      "Epoch [6/20], Step [100/782], Loss: 4.0410\n",
      "Epoch [6/20], Step [200/782], Loss: 3.9891\n",
      "Epoch [6/20], Step [300/782], Loss: 3.7981\n",
      "Epoch [6/20], Step [400/782], Loss: 3.8315\n",
      "Epoch [6/20], Step [500/782], Loss: 4.1326\n",
      "Epoch [6/20], Step [600/782], Loss: 4.1715\n",
      "Epoch [6/20], Step [700/782], Loss: 4.1759\n",
      "Epoch 6 completed in 21.95 seconds\n",
      "Epoch [7/20], Step [100/782], Loss: 3.8844\n",
      "Epoch [7/20], Step [200/782], Loss: 4.0793\n",
      "Epoch [7/20], Step [300/782], Loss: 4.0567\n",
      "Epoch [7/20], Step [400/782], Loss: 3.9772\n",
      "Epoch [7/20], Step [500/782], Loss: 3.7762\n",
      "Epoch [7/20], Step [600/782], Loss: 3.7728\n",
      "Epoch [7/20], Step [700/782], Loss: 3.7793\n",
      "Epoch 7 completed in 22.10 seconds\n",
      "Epoch [8/20], Step [100/782], Loss: 4.4442\n",
      "Epoch [8/20], Step [200/782], Loss: 3.9051\n",
      "Epoch [8/20], Step [300/782], Loss: 3.9675\n",
      "Epoch [8/20], Step [400/782], Loss: 3.9524\n",
      "Epoch [8/20], Step [500/782], Loss: 3.7196\n",
      "Epoch [8/20], Step [600/782], Loss: 3.7589\n",
      "Epoch [8/20], Step [700/782], Loss: 3.9894\n",
      "Epoch 8 completed in 22.20 seconds\n",
      "Epoch [9/20], Step [100/782], Loss: 3.8101\n",
      "Epoch [9/20], Step [200/782], Loss: 3.7250\n",
      "Epoch [9/20], Step [300/782], Loss: 4.1187\n",
      "Epoch [9/20], Step [400/782], Loss: 3.8132\n",
      "Epoch [9/20], Step [500/782], Loss: 4.0868\n",
      "Epoch [9/20], Step [600/782], Loss: 3.7977\n",
      "Epoch [9/20], Step [700/782], Loss: 3.7135\n",
      "Epoch 9 completed in 21.74 seconds\n",
      "Epoch [10/20], Step [100/782], Loss: 4.0381\n",
      "Epoch [10/20], Step [200/782], Loss: 3.8282\n",
      "Epoch [10/20], Step [300/782], Loss: 3.7405\n",
      "Epoch [10/20], Step [400/782], Loss: 3.7202\n",
      "Epoch [10/20], Step [500/782], Loss: 3.9697\n",
      "Epoch [10/20], Step [600/782], Loss: 4.1043\n",
      "Epoch [10/20], Step [700/782], Loss: 3.8576\n",
      "Epoch 10 completed in 21.76 seconds\n",
      "Epoch [11/20], Step [100/782], Loss: 3.6735\n",
      "Epoch [11/20], Step [200/782], Loss: 3.9434\n",
      "Epoch [11/20], Step [300/782], Loss: 3.9427\n",
      "Epoch [11/20], Step [400/782], Loss: 3.8682\n",
      "Epoch [11/20], Step [500/782], Loss: 4.0822\n",
      "Epoch [11/20], Step [600/782], Loss: 3.7210\n",
      "Epoch [11/20], Step [700/782], Loss: 3.9416\n",
      "Epoch 11 completed in 21.81 seconds\n",
      "Epoch [12/20], Step [100/782], Loss: 3.9552\n",
      "Epoch [12/20], Step [200/782], Loss: 4.0781\n",
      "Epoch [12/20], Step [300/782], Loss: 4.0727\n",
      "Epoch [12/20], Step [400/782], Loss: 4.0494\n",
      "Epoch [12/20], Step [500/782], Loss: 3.8311\n",
      "Epoch [12/20], Step [600/782], Loss: 3.6957\n",
      "Epoch [12/20], Step [700/782], Loss: 4.0243\n",
      "Epoch 12 completed in 22.12 seconds\n",
      "Epoch [13/20], Step [100/782], Loss: 3.7410\n",
      "Epoch [13/20], Step [200/782], Loss: 4.0800\n",
      "Epoch [13/20], Step [300/782], Loss: 3.8126\n",
      "Epoch [13/20], Step [400/782], Loss: 3.8743\n",
      "Epoch [13/20], Step [500/782], Loss: 3.9513\n",
      "Epoch [13/20], Step [600/782], Loss: 3.7195\n",
      "Epoch [13/20], Step [700/782], Loss: 3.8012\n",
      "Epoch 13 completed in 21.39 seconds\n",
      "Epoch [14/20], Step [100/782], Loss: 3.4871\n",
      "Epoch [14/20], Step [200/782], Loss: 3.8763\n",
      "Epoch [14/20], Step [300/782], Loss: 3.9040\n",
      "Epoch [14/20], Step [400/782], Loss: 3.9690\n",
      "Epoch [14/20], Step [500/782], Loss: 3.6417\n",
      "Epoch [14/20], Step [600/782], Loss: 3.8259\n",
      "Epoch [14/20], Step [700/782], Loss: 3.8974\n",
      "Epoch 14 completed in 22.00 seconds\n",
      "Epoch [15/20], Step [100/782], Loss: 3.6357\n",
      "Epoch [15/20], Step [200/782], Loss: 3.5463\n",
      "Epoch [15/20], Step [300/782], Loss: 4.0020\n",
      "Epoch [15/20], Step [400/782], Loss: 4.2584\n",
      "Epoch [15/20], Step [500/782], Loss: 3.7210\n",
      "Epoch [15/20], Step [600/782], Loss: 4.1359\n",
      "Epoch [15/20], Step [700/782], Loss: 3.8625\n",
      "Epoch 15 completed in 22.22 seconds\n",
      "Epoch [16/20], Step [100/782], Loss: 4.2477\n",
      "Epoch [16/20], Step [200/782], Loss: 3.7302\n",
      "Epoch [16/20], Step [300/782], Loss: 3.5317\n",
      "Epoch [16/20], Step [400/782], Loss: 3.4731\n",
      "Epoch [16/20], Step [500/782], Loss: 3.8150\n",
      "Epoch [16/20], Step [600/782], Loss: 3.6624\n",
      "Epoch [16/20], Step [700/782], Loss: 4.0925\n",
      "Epoch 16 completed in 22.01 seconds\n",
      "Epoch [17/20], Step [100/782], Loss: 3.7229\n",
      "Epoch [17/20], Step [200/782], Loss: 4.1984\n",
      "Epoch [17/20], Step [300/782], Loss: 3.9467\n",
      "Epoch [17/20], Step [400/782], Loss: 4.1550\n",
      "Epoch [17/20], Step [500/782], Loss: 3.6824\n",
      "Epoch [17/20], Step [600/782], Loss: 3.7664\n",
      "Epoch [17/20], Step [700/782], Loss: 3.7419\n",
      "Epoch 17 completed in 22.17 seconds\n",
      "Epoch [18/20], Step [100/782], Loss: 3.6114\n",
      "Epoch [18/20], Step [200/782], Loss: 3.4778\n",
      "Epoch [18/20], Step [300/782], Loss: 3.4525\n",
      "Epoch [18/20], Step [400/782], Loss: 3.8811\n",
      "Epoch [18/20], Step [500/782], Loss: 3.6819\n",
      "Epoch [18/20], Step [600/782], Loss: 3.6278\n",
      "Epoch [18/20], Step [700/782], Loss: 3.5223\n",
      "Epoch 18 completed in 22.08 seconds\n",
      "Epoch [19/20], Step [100/782], Loss: 3.8336\n",
      "Epoch [19/20], Step [200/782], Loss: 3.8803\n",
      "Epoch [19/20], Step [300/782], Loss: 3.8582\n",
      "Epoch [19/20], Step [400/782], Loss: 3.7255\n",
      "Epoch [19/20], Step [500/782], Loss: 3.9972\n",
      "Epoch [19/20], Step [600/782], Loss: 3.9191\n",
      "Epoch [19/20], Step [700/782], Loss: 3.9294\n",
      "Epoch 19 completed in 21.55 seconds\n",
      "Epoch [20/20], Step [100/782], Loss: 3.6996\n",
      "Epoch [20/20], Step [200/782], Loss: 3.7346\n",
      "Epoch [20/20], Step [300/782], Loss: 3.9938\n",
      "Epoch [20/20], Step [400/782], Loss: 3.7012\n",
      "Epoch [20/20], Step [500/782], Loss: 3.9595\n",
      "Epoch [20/20], Step [600/782], Loss: 3.5773\n",
      "Epoch [20/20], Step [700/782], Loss: 3.6214\n",
      "Epoch 20 completed in 20.85 seconds\n",
      "Test Accuracy: 10.88%\n",
      "\n",
      "Training ViT-Medium configuration...\n",
      "Patch size: 4, Embed dim: 512, Depth: 4, Heads: 4, MLP ratio: 4\n",
      "\n",
      "Model Summary:\n",
      "Epoch [1/20], Step [100/782], Loss: 4.4071\n",
      "Epoch [1/20], Step [200/782], Loss: 4.4310\n",
      "Epoch [1/20], Step [300/782], Loss: 4.1514\n",
      "Epoch [1/20], Step [400/782], Loss: 3.8304\n",
      "Epoch [1/20], Step [500/782], Loss: 3.9177\n",
      "Epoch [1/20], Step [600/782], Loss: 4.3529\n",
      "Epoch [1/20], Step [700/782], Loss: 4.1007\n",
      "Epoch 1 completed in 20.73 seconds\n",
      "Epoch [2/20], Step [100/782], Loss: 4.1946\n",
      "Epoch [2/20], Step [200/782], Loss: 4.0626\n",
      "Epoch [2/20], Step [300/782], Loss: 4.2985\n",
      "Epoch [2/20], Step [400/782], Loss: 4.2319\n",
      "Epoch [2/20], Step [500/782], Loss: 4.0737\n",
      "Epoch [2/20], Step [600/782], Loss: 3.9356\n",
      "Epoch [2/20], Step [700/782], Loss: 4.1009\n",
      "Epoch 2 completed in 20.85 seconds\n",
      "Epoch [3/20], Step [100/782], Loss: 4.0712\n",
      "Epoch [3/20], Step [200/782], Loss: 3.7297\n",
      "Epoch [3/20], Step [300/782], Loss: 3.9585\n",
      "Epoch [3/20], Step [400/782], Loss: 3.9618\n",
      "Epoch [3/20], Step [500/782], Loss: 4.0961\n",
      "Epoch [3/20], Step [600/782], Loss: 4.0776\n",
      "Epoch [3/20], Step [700/782], Loss: 4.0664\n",
      "Epoch 3 completed in 20.86 seconds\n",
      "Epoch [4/20], Step [100/782], Loss: 3.9797\n",
      "Epoch [4/20], Step [200/782], Loss: 3.8200\n",
      "Epoch [4/20], Step [300/782], Loss: 4.0012\n",
      "Epoch [4/20], Step [400/782], Loss: 4.0735\n",
      "Epoch [4/20], Step [500/782], Loss: 4.0210\n",
      "Epoch [4/20], Step [600/782], Loss: 4.0367\n",
      "Epoch [4/20], Step [700/782], Loss: 3.8648\n",
      "Epoch 4 completed in 20.80 seconds\n",
      "Epoch [5/20], Step [100/782], Loss: 3.8318\n",
      "Epoch [5/20], Step [200/782], Loss: 3.9527\n",
      "Epoch [5/20], Step [300/782], Loss: 3.8294\n",
      "Epoch [5/20], Step [400/782], Loss: 3.3955\n",
      "Epoch [5/20], Step [500/782], Loss: 3.6954\n",
      "Epoch [5/20], Step [600/782], Loss: 3.7042\n",
      "Epoch [5/20], Step [700/782], Loss: 3.7492\n",
      "Epoch 5 completed in 21.51 seconds\n",
      "Epoch [6/20], Step [100/782], Loss: 3.8560\n",
      "Epoch [6/20], Step [200/782], Loss: 3.9091\n",
      "Epoch [6/20], Step [300/782], Loss: 3.9273\n",
      "Epoch [6/20], Step [400/782], Loss: 3.9215\n",
      "Epoch [6/20], Step [500/782], Loss: 3.6839\n",
      "Epoch [6/20], Step [600/782], Loss: 3.9312\n",
      "Epoch [6/20], Step [700/782], Loss: 3.9684\n",
      "Epoch 6 completed in 21.25 seconds\n",
      "Epoch [7/20], Step [100/782], Loss: 3.6402\n",
      "Epoch [7/20], Step [200/782], Loss: 3.9486\n",
      "Epoch [7/20], Step [300/782], Loss: 3.7814\n",
      "Epoch [7/20], Step [400/782], Loss: 3.8900\n",
      "Epoch [7/20], Step [500/782], Loss: 4.2010\n",
      "Epoch [7/20], Step [600/782], Loss: 3.7954\n",
      "Epoch [7/20], Step [700/782], Loss: 3.6951\n",
      "Epoch 7 completed in 21.38 seconds\n",
      "Epoch [8/20], Step [100/782], Loss: 4.0915\n",
      "Epoch [8/20], Step [200/782], Loss: 3.8273\n",
      "Epoch [8/20], Step [300/782], Loss: 4.0291\n",
      "Epoch [8/20], Step [400/782], Loss: 3.8202\n",
      "Epoch [8/20], Step [500/782], Loss: 3.8066\n",
      "Epoch [8/20], Step [600/782], Loss: 4.0795\n",
      "Epoch [8/20], Step [700/782], Loss: 4.0749\n",
      "Epoch 8 completed in 21.15 seconds\n",
      "Epoch [9/20], Step [100/782], Loss: 3.6934\n",
      "Epoch [9/20], Step [200/782], Loss: 4.0095\n",
      "Epoch [9/20], Step [300/782], Loss: 3.8021\n",
      "Epoch [9/20], Step [400/782], Loss: 3.9938\n",
      "Epoch [9/20], Step [500/782], Loss: 3.9081\n",
      "Epoch [9/20], Step [600/782], Loss: 4.0719\n",
      "Epoch [9/20], Step [700/782], Loss: 3.8728\n",
      "Epoch 9 completed in 21.35 seconds\n",
      "Epoch [10/20], Step [100/782], Loss: 3.8371\n",
      "Epoch [10/20], Step [200/782], Loss: 3.7265\n",
      "Epoch [10/20], Step [300/782], Loss: 3.7787\n",
      "Epoch [10/20], Step [400/782], Loss: 4.1545\n",
      "Epoch [10/20], Step [500/782], Loss: 3.9329\n",
      "Epoch [10/20], Step [600/782], Loss: 4.0250\n",
      "Epoch [10/20], Step [700/782], Loss: 4.0338\n",
      "Epoch 10 completed in 21.37 seconds\n",
      "Epoch [11/20], Step [100/782], Loss: 3.8774\n",
      "Epoch [11/20], Step [200/782], Loss: 3.6708\n",
      "Epoch [11/20], Step [300/782], Loss: 3.9370\n",
      "Epoch [11/20], Step [400/782], Loss: 4.0852\n",
      "Epoch [11/20], Step [500/782], Loss: 3.9884\n",
      "Epoch [11/20], Step [600/782], Loss: 4.1061\n",
      "Epoch [11/20], Step [700/782], Loss: 3.8126\n",
      "Epoch 11 completed in 21.33 seconds\n",
      "Epoch [12/20], Step [100/782], Loss: 3.9010\n",
      "Epoch [12/20], Step [200/782], Loss: 4.2276\n",
      "Epoch [12/20], Step [300/782], Loss: 3.9727\n",
      "Epoch [12/20], Step [400/782], Loss: 4.1202\n",
      "Epoch [12/20], Step [500/782], Loss: 3.7733\n",
      "Epoch [12/20], Step [600/782], Loss: 4.0875\n",
      "Epoch [12/20], Step [700/782], Loss: 3.9313\n",
      "Epoch 12 completed in 21.22 seconds\n",
      "Epoch [13/20], Step [100/782], Loss: 3.9484\n",
      "Epoch [13/20], Step [200/782], Loss: 4.0911\n",
      "Epoch [13/20], Step [300/782], Loss: 4.1686\n",
      "Epoch [13/20], Step [400/782], Loss: 4.1559\n",
      "Epoch [13/20], Step [500/782], Loss: 3.9600\n",
      "Epoch [13/20], Step [600/782], Loss: 4.0125\n",
      "Epoch [13/20], Step [700/782], Loss: 4.0780\n",
      "Epoch 13 completed in 21.28 seconds\n",
      "Epoch [14/20], Step [100/782], Loss: 4.0611\n",
      "Epoch [14/20], Step [200/782], Loss: 3.9162\n",
      "Epoch [14/20], Step [300/782], Loss: 4.1697\n",
      "Epoch [14/20], Step [400/782], Loss: 4.0446\n",
      "Epoch [14/20], Step [500/782], Loss: 4.1169\n",
      "Epoch [14/20], Step [600/782], Loss: 3.8475\n",
      "Epoch [14/20], Step [700/782], Loss: 3.7266\n",
      "Epoch 14 completed in 21.09 seconds\n",
      "Epoch [15/20], Step [100/782], Loss: 3.8743\n",
      "Epoch [15/20], Step [200/782], Loss: 4.1845\n",
      "Epoch [15/20], Step [300/782], Loss: 3.9745\n",
      "Epoch [15/20], Step [400/782], Loss: 3.7097\n",
      "Epoch [15/20], Step [500/782], Loss: 3.9069\n",
      "Epoch [15/20], Step [600/782], Loss: 3.7957\n",
      "Epoch [15/20], Step [700/782], Loss: 4.1457\n",
      "Epoch 15 completed in 21.24 seconds\n",
      "Epoch [16/20], Step [100/782], Loss: 3.9428\n",
      "Epoch [16/20], Step [200/782], Loss: 3.7918\n",
      "Epoch [16/20], Step [300/782], Loss: 3.7789\n",
      "Epoch [16/20], Step [400/782], Loss: 3.8258\n",
      "Epoch [16/20], Step [500/782], Loss: 3.8114\n",
      "Epoch [16/20], Step [600/782], Loss: 3.8395\n",
      "Epoch [16/20], Step [700/782], Loss: 3.7926\n",
      "Epoch 16 completed in 21.27 seconds\n",
      "Epoch [17/20], Step [100/782], Loss: 4.0705\n",
      "Epoch [17/20], Step [200/782], Loss: 4.0380\n",
      "Epoch [17/20], Step [300/782], Loss: 3.9175\n",
      "Epoch [17/20], Step [400/782], Loss: 3.8066\n",
      "Epoch [17/20], Step [500/782], Loss: 3.7355\n",
      "Epoch [17/20], Step [600/782], Loss: 4.0331\n",
      "Epoch [17/20], Step [700/782], Loss: 3.9847\n",
      "Epoch 17 completed in 21.30 seconds\n",
      "Epoch [18/20], Step [100/782], Loss: 3.7932\n",
      "Epoch [18/20], Step [200/782], Loss: 3.9244\n",
      "Epoch [18/20], Step [300/782], Loss: 4.1769\n",
      "Epoch [18/20], Step [400/782], Loss: 3.7394\n",
      "Epoch [18/20], Step [500/782], Loss: 4.2637\n",
      "Epoch [18/20], Step [600/782], Loss: 3.8150\n",
      "Epoch [18/20], Step [700/782], Loss: 4.1571\n",
      "Epoch 18 completed in 20.75 seconds\n",
      "Epoch [19/20], Step [100/782], Loss: 3.9781\n",
      "Epoch [19/20], Step [200/782], Loss: 3.8573\n",
      "Epoch [19/20], Step [300/782], Loss: 3.7757\n",
      "Epoch [19/20], Step [400/782], Loss: 4.2485\n",
      "Epoch [19/20], Step [500/782], Loss: 3.7310\n",
      "Epoch [19/20], Step [600/782], Loss: 3.7608\n",
      "Epoch [19/20], Step [700/782], Loss: 3.6181\n",
      "Epoch 19 completed in 21.19 seconds\n",
      "Epoch [20/20], Step [100/782], Loss: 4.0999\n",
      "Epoch [20/20], Step [200/782], Loss: 4.0146\n",
      "Epoch [20/20], Step [300/782], Loss: 4.0435\n",
      "Epoch [20/20], Step [400/782], Loss: 3.7251\n",
      "Epoch [20/20], Step [500/782], Loss: 3.9489\n",
      "Epoch [20/20], Step [600/782], Loss: 3.8671\n",
      "Epoch [20/20], Step [700/782], Loss: 3.8483\n",
      "Epoch 20 completed in 20.91 seconds\n",
      "Test Accuracy: 10.12%\n",
      "\n",
      "Training ViT-Large configuration...\n",
      "Patch size: 8, Embed dim: 512, Depth: 8, Heads: 4, MLP ratio: 4\n",
      "\n",
      "Model Summary:\n",
      "Epoch [1/20], Step [100/782], Loss: 4.4389\n",
      "Epoch [1/20], Step [200/782], Loss: 4.4962\n",
      "Epoch [1/20], Step [300/782], Loss: 4.2514\n",
      "Epoch [1/20], Step [400/782], Loss: 4.3006\n",
      "Epoch [1/20], Step [500/782], Loss: 4.2342\n",
      "Epoch [1/20], Step [600/782], Loss: 4.3535\n",
      "Epoch [1/20], Step [700/782], Loss: 4.3957\n",
      "Epoch 1 completed in 21.75 seconds\n",
      "Epoch [2/20], Step [100/782], Loss: 4.3185\n",
      "Epoch [2/20], Step [200/782], Loss: 4.2171\n",
      "Epoch [2/20], Step [300/782], Loss: 4.0930\n",
      "Epoch [2/20], Step [400/782], Loss: 4.3768\n",
      "Epoch [2/20], Step [500/782], Loss: 4.4339\n",
      "Epoch [2/20], Step [600/782], Loss: 4.0199\n",
      "Epoch [2/20], Step [700/782], Loss: 4.0916\n",
      "Epoch 2 completed in 21.38 seconds\n",
      "Epoch [3/20], Step [100/782], Loss: 4.0620\n",
      "Epoch [3/20], Step [200/782], Loss: 4.1696\n",
      "Epoch [3/20], Step [300/782], Loss: 4.3064\n",
      "Epoch [3/20], Step [400/782], Loss: 4.2094\n",
      "Epoch [3/20], Step [500/782], Loss: 4.1063\n",
      "Epoch [3/20], Step [600/782], Loss: 4.0449\n",
      "Epoch [3/20], Step [700/782], Loss: 4.1336\n",
      "Epoch 3 completed in 21.06 seconds\n",
      "Epoch [4/20], Step [100/782], Loss: 3.9339\n",
      "Epoch [4/20], Step [200/782], Loss: 4.1213\n",
      "Epoch [4/20], Step [300/782], Loss: 4.0410\n",
      "Epoch [4/20], Step [400/782], Loss: 4.2192\n",
      "Epoch [4/20], Step [500/782], Loss: 4.1195\n",
      "Epoch [4/20], Step [600/782], Loss: 4.0975\n",
      "Epoch [4/20], Step [700/782], Loss: 3.9407\n",
      "Epoch 4 completed in 21.30 seconds\n",
      "Epoch [5/20], Step [100/782], Loss: 4.1468\n",
      "Epoch [5/20], Step [200/782], Loss: 4.1106\n",
      "Epoch [5/20], Step [300/782], Loss: 4.0571\n",
      "Epoch [5/20], Step [400/782], Loss: 4.1439\n",
      "Epoch [5/20], Step [500/782], Loss: 4.1650\n",
      "Epoch [5/20], Step [600/782], Loss: 3.8711\n",
      "Epoch [5/20], Step [700/782], Loss: 4.0361\n",
      "Epoch 5 completed in 21.56 seconds\n",
      "Epoch [6/20], Step [100/782], Loss: 4.0523\n",
      "Epoch [6/20], Step [200/782], Loss: 4.0989\n",
      "Epoch [6/20], Step [300/782], Loss: 4.0095\n",
      "Epoch [6/20], Step [400/782], Loss: 3.8080\n",
      "Epoch [6/20], Step [500/782], Loss: 3.9407\n",
      "Epoch [6/20], Step [600/782], Loss: 4.1164\n",
      "Epoch [6/20], Step [700/782], Loss: 3.8820\n",
      "Epoch 6 completed in 20.93 seconds\n",
      "Epoch [7/20], Step [100/782], Loss: 4.1969\n",
      "Epoch [7/20], Step [200/782], Loss: 3.9983\n",
      "Epoch [7/20], Step [300/782], Loss: 3.9636\n",
      "Epoch [7/20], Step [400/782], Loss: 4.1883\n",
      "Epoch [7/20], Step [500/782], Loss: 4.0598\n",
      "Epoch [7/20], Step [600/782], Loss: 3.9583\n",
      "Epoch [7/20], Step [700/782], Loss: 3.9173\n",
      "Epoch 7 completed in 21.08 seconds\n",
      "Epoch [8/20], Step [100/782], Loss: 3.9628\n",
      "Epoch [8/20], Step [200/782], Loss: 4.1568\n",
      "Epoch [8/20], Step [300/782], Loss: 4.1664\n",
      "Epoch [8/20], Step [400/782], Loss: 4.0470\n",
      "Epoch [8/20], Step [500/782], Loss: 3.8397\n",
      "Epoch [8/20], Step [600/782], Loss: 3.9403\n",
      "Epoch [8/20], Step [700/782], Loss: 4.0534\n",
      "Epoch 8 completed in 20.94 seconds\n",
      "Epoch [9/20], Step [100/782], Loss: 4.0163\n",
      "Epoch [9/20], Step [200/782], Loss: 4.0372\n",
      "Epoch [9/20], Step [300/782], Loss: 3.9767\n",
      "Epoch [9/20], Step [400/782], Loss: 3.9754\n",
      "Epoch [9/20], Step [500/782], Loss: 4.1749\n",
      "Epoch [9/20], Step [600/782], Loss: 3.9639\n",
      "Epoch [9/20], Step [700/782], Loss: 4.1381\n",
      "Epoch 9 completed in 20.90 seconds\n",
      "Epoch [10/20], Step [100/782], Loss: 4.0570\n",
      "Epoch [10/20], Step [200/782], Loss: 3.9857\n",
      "Epoch [10/20], Step [300/782], Loss: 4.0351\n",
      "Epoch [10/20], Step [400/782], Loss: 4.0299\n",
      "Epoch [10/20], Step [500/782], Loss: 4.0182\n",
      "Epoch [10/20], Step [600/782], Loss: 4.2198\n",
      "Epoch [10/20], Step [700/782], Loss: 3.9506\n",
      "Epoch 10 completed in 20.96 seconds\n",
      "Epoch [11/20], Step [100/782], Loss: 4.1256\n",
      "Epoch [11/20], Step [200/782], Loss: 4.1451\n",
      "Epoch [11/20], Step [300/782], Loss: 4.0495\n",
      "Epoch [11/20], Step [400/782], Loss: 4.0101\n",
      "Epoch [11/20], Step [500/782], Loss: 3.8447\n",
      "Epoch [11/20], Step [600/782], Loss: 4.2594\n",
      "Epoch [11/20], Step [700/782], Loss: 4.1861\n",
      "Epoch 11 completed in 20.95 seconds\n",
      "Epoch [12/20], Step [100/782], Loss: 3.9085\n",
      "Epoch [12/20], Step [200/782], Loss: 3.8923\n",
      "Epoch [12/20], Step [300/782], Loss: 4.0316\n",
      "Epoch [12/20], Step [400/782], Loss: 4.1047\n",
      "Epoch [12/20], Step [500/782], Loss: 4.0782\n",
      "Epoch [12/20], Step [600/782], Loss: 3.9357\n",
      "Epoch [12/20], Step [700/782], Loss: 4.1888\n",
      "Epoch 12 completed in 21.13 seconds\n",
      "Epoch [13/20], Step [100/782], Loss: 4.0396\n",
      "Epoch [13/20], Step [200/782], Loss: 4.0250\n",
      "Epoch [13/20], Step [300/782], Loss: 4.4581\n",
      "Epoch [13/20], Step [400/782], Loss: 4.2561\n",
      "Epoch [13/20], Step [500/782], Loss: 4.1528\n",
      "Epoch [13/20], Step [600/782], Loss: 4.1778\n",
      "Epoch [13/20], Step [700/782], Loss: 4.0155\n",
      "Epoch 13 completed in 21.04 seconds\n",
      "Epoch [14/20], Step [100/782], Loss: 3.8518\n",
      "Epoch [14/20], Step [200/782], Loss: 3.9832\n",
      "Epoch [14/20], Step [300/782], Loss: 4.2542\n",
      "Epoch [14/20], Step [400/782], Loss: 4.2000\n",
      "Epoch [14/20], Step [500/782], Loss: 3.9229\n",
      "Epoch [14/20], Step [600/782], Loss: 3.9460\n",
      "Epoch [14/20], Step [700/782], Loss: 4.2147\n",
      "Epoch 14 completed in 22.05 seconds\n",
      "Epoch [15/20], Step [100/782], Loss: 4.2153\n",
      "Epoch [15/20], Step [200/782], Loss: 4.1812\n",
      "Epoch [15/20], Step [300/782], Loss: 4.1496\n",
      "Epoch [15/20], Step [400/782], Loss: 3.9936\n",
      "Epoch [15/20], Step [500/782], Loss: 4.2730\n",
      "Epoch [15/20], Step [600/782], Loss: 3.7767\n",
      "Epoch [15/20], Step [700/782], Loss: 4.2961\n",
      "Epoch 15 completed in 21.22 seconds\n",
      "Epoch [16/20], Step [100/782], Loss: 4.2069\n",
      "Epoch [16/20], Step [200/782], Loss: 4.2618\n",
      "Epoch [16/20], Step [300/782], Loss: 4.1418\n",
      "Epoch [16/20], Step [400/782], Loss: 4.2132\n",
      "Epoch [16/20], Step [500/782], Loss: 4.1603\n",
      "Epoch [16/20], Step [600/782], Loss: 4.0124\n",
      "Epoch [16/20], Step [700/782], Loss: 3.9790\n",
      "Epoch 16 completed in 21.41 seconds\n",
      "Epoch [17/20], Step [100/782], Loss: 4.2280\n",
      "Epoch [17/20], Step [200/782], Loss: 4.2305\n",
      "Epoch [17/20], Step [300/782], Loss: 4.1792\n",
      "Epoch [17/20], Step [400/782], Loss: 4.1454\n",
      "Epoch [17/20], Step [500/782], Loss: 4.0994\n",
      "Epoch [17/20], Step [600/782], Loss: 3.9863\n",
      "Epoch [17/20], Step [700/782], Loss: 4.2688\n",
      "Epoch 17 completed in 21.15 seconds\n",
      "Epoch [18/20], Step [100/782], Loss: 4.1097\n",
      "Epoch [18/20], Step [200/782], Loss: 3.9818\n",
      "Epoch [18/20], Step [300/782], Loss: 4.0925\n",
      "Epoch [18/20], Step [400/782], Loss: 4.1160\n",
      "Epoch [18/20], Step [500/782], Loss: 4.2467\n",
      "Epoch [18/20], Step [600/782], Loss: 3.8225\n",
      "Epoch [18/20], Step [700/782], Loss: 3.7931\n",
      "Epoch 18 completed in 21.33 seconds\n",
      "Epoch [19/20], Step [100/782], Loss: 4.1276\n",
      "Epoch [19/20], Step [200/782], Loss: 4.0719\n",
      "Epoch [19/20], Step [300/782], Loss: 3.6609\n",
      "Epoch [19/20], Step [400/782], Loss: 4.1181\n",
      "Epoch [19/20], Step [500/782], Loss: 4.3007\n",
      "Epoch [19/20], Step [600/782], Loss: 3.9181\n",
      "Epoch [19/20], Step [700/782], Loss: 4.1904\n",
      "Epoch 19 completed in 20.96 seconds\n",
      "Epoch [20/20], Step [100/782], Loss: 4.1724\n",
      "Epoch [20/20], Step [200/782], Loss: 3.9664\n",
      "Epoch [20/20], Step [300/782], Loss: 4.1885\n",
      "Epoch [20/20], Step [400/782], Loss: 4.0570\n",
      "Epoch [20/20], Step [500/782], Loss: 4.0303\n",
      "Epoch [20/20], Step [600/782], Loss: 4.0275\n",
      "Epoch [20/20], Step [700/782], Loss: 4.1095\n",
      "Epoch 20 completed in 21.78 seconds\n",
      "Test Accuracy: 7.64%\n",
      "\n",
      "Training ResNet-18 baseline...\n",
      "\n",
      "ResNet-18 Summary:\n",
      "Epoch [1/10], Step [100/782], Loss: 4.0928\n",
      "Epoch [1/10], Step [200/782], Loss: 3.6711\n",
      "Epoch [1/10], Step [300/782], Loss: 3.7856\n",
      "Epoch [1/10], Step [400/782], Loss: 3.3562\n",
      "Epoch [1/10], Step [500/782], Loss: 3.6083\n",
      "Epoch [1/10], Step [600/782], Loss: 3.1535\n",
      "Epoch [1/10], Step [700/782], Loss: 3.1159\n",
      "Epoch 1 completed in 15.40 seconds\n",
      "Epoch [2/10], Step [100/782], Loss: 2.8890\n",
      "Epoch [2/10], Step [200/782], Loss: 2.8343\n",
      "Epoch [2/10], Step [300/782], Loss: 2.8344\n",
      "Epoch [2/10], Step [400/782], Loss: 2.5202\n",
      "Epoch [2/10], Step [500/782], Loss: 2.7314\n",
      "Epoch [2/10], Step [600/782], Loss: 2.3839\n",
      "Epoch [2/10], Step [700/782], Loss: 2.2169\n",
      "Epoch 2 completed in 14.78 seconds\n",
      "Epoch [3/10], Step [100/782], Loss: 2.1831\n",
      "Epoch [3/10], Step [200/782], Loss: 2.7389\n",
      "Epoch [3/10], Step [300/782], Loss: 2.5408\n",
      "Epoch [3/10], Step [400/782], Loss: 2.1782\n",
      "Epoch [3/10], Step [500/782], Loss: 2.3069\n",
      "Epoch [3/10], Step [600/782], Loss: 2.0738\n",
      "Epoch [3/10], Step [700/782], Loss: 1.7082\n",
      "Epoch 3 completed in 14.64 seconds\n",
      "Epoch [4/10], Step [100/782], Loss: 2.1135\n",
      "Epoch [4/10], Step [200/782], Loss: 1.7734\n",
      "Epoch [4/10], Step [300/782], Loss: 1.9619\n",
      "Epoch [4/10], Step [400/782], Loss: 1.8449\n",
      "Epoch [4/10], Step [500/782], Loss: 2.0346\n",
      "Epoch [4/10], Step [600/782], Loss: 1.7819\n",
      "Epoch [4/10], Step [700/782], Loss: 1.9767\n",
      "Epoch 4 completed in 14.70 seconds\n",
      "Epoch [5/10], Step [100/782], Loss: 1.4662\n",
      "Epoch [5/10], Step [200/782], Loss: 1.7788\n",
      "Epoch [5/10], Step [300/782], Loss: 1.9250\n",
      "Epoch [5/10], Step [400/782], Loss: 1.7192\n",
      "Epoch [5/10], Step [500/782], Loss: 1.5323\n",
      "Epoch [5/10], Step [600/782], Loss: 1.7740\n",
      "Epoch [5/10], Step [700/782], Loss: 2.0960\n",
      "Epoch 5 completed in 14.71 seconds\n",
      "Epoch [6/10], Step [100/782], Loss: 1.3574\n",
      "Epoch [6/10], Step [200/782], Loss: 1.4239\n",
      "Epoch [6/10], Step [300/782], Loss: 1.6239\n",
      "Epoch [6/10], Step [400/782], Loss: 1.3694\n",
      "Epoch [6/10], Step [500/782], Loss: 1.4776\n",
      "Epoch [6/10], Step [600/782], Loss: 1.6943\n",
      "Epoch [6/10], Step [700/782], Loss: 1.4082\n",
      "Epoch 6 completed in 14.69 seconds\n",
      "Epoch [7/10], Step [100/782], Loss: 1.3161\n",
      "Epoch [7/10], Step [200/782], Loss: 1.4261\n",
      "Epoch [7/10], Step [300/782], Loss: 1.1326\n",
      "Epoch [7/10], Step [400/782], Loss: 1.6536\n",
      "Epoch [7/10], Step [500/782], Loss: 1.3489\n",
      "Epoch [7/10], Step [600/782], Loss: 1.2786\n",
      "Epoch [7/10], Step [700/782], Loss: 1.5910\n",
      "Epoch 7 completed in 14.93 seconds\n",
      "Epoch [8/10], Step [100/782], Loss: 0.7192\n",
      "Epoch [8/10], Step [200/782], Loss: 0.8847\n",
      "Epoch [8/10], Step [300/782], Loss: 1.0642\n",
      "Epoch [8/10], Step [400/782], Loss: 1.1078\n",
      "Epoch [8/10], Step [500/782], Loss: 1.0071\n",
      "Epoch [8/10], Step [600/782], Loss: 1.2662\n",
      "Epoch [8/10], Step [700/782], Loss: 1.0522\n",
      "Epoch 8 completed in 15.26 seconds\n",
      "Epoch [9/10], Step [100/782], Loss: 0.9064\n",
      "Epoch [9/10], Step [200/782], Loss: 0.7113\n",
      "Epoch [9/10], Step [300/782], Loss: 0.7792\n",
      "Epoch [9/10], Step [400/782], Loss: 0.5257\n",
      "Epoch [9/10], Step [500/782], Loss: 1.0122\n",
      "Epoch [9/10], Step [600/782], Loss: 0.6813\n",
      "Epoch [9/10], Step [700/782], Loss: 0.9867\n",
      "Epoch 9 completed in 14.90 seconds\n",
      "Epoch [10/10], Step [100/782], Loss: 0.4502\n",
      "Epoch [10/10], Step [200/782], Loss: 0.5040\n",
      "Epoch [10/10], Step [300/782], Loss: 0.4162\n",
      "Epoch [10/10], Step [400/782], Loss: 0.6607\n",
      "Epoch [10/10], Step [500/782], Loss: 0.6535\n",
      "Epoch [10/10], Step [600/782], Loss: 0.6146\n",
      "Epoch [10/10], Step [700/782], Loss: 1.0534\n",
      "Epoch 10 completed in 14.62 seconds\n",
      "Test Accuracy: 44.83%\n",
      "\n",
      "Results Summary:\n",
      "========================================================================================================================\n",
      "Model          Patch   Embed   Depth   Heads   MLP     Params         FLOPs          Time/Epoch     Accuracy  \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "ViT-Tiny       4       256     4       2       2       2.16M     4.43G     15.28s       18.58%\n",
      "ViT-Small      8       256     8       2       2       4.30M     8.80G     21.87s       10.88%\n",
      "ViT-Medium     4       512     4       4       4       12.72M     26.05G     21.14s       10.12%\n",
      "ViT-Large      8       512     8       4       4       25.38M     51.98G     21.24s       7.64%\n",
      "ResNet-18      N/A     N/A     18      N/A     N/A     11.23M     22.99G     14.86s       44.83%\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "'''Problem 1\n",
    "'''\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "num_classes = 100\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "\n",
    "# Data loading and preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))  # CIFAR-100 stats\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR100(\n",
    "    root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.CIFAR100(\n",
    "    root='./data', train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Vision Transformer (ViT) implementation\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=32, patch_size=4, in_channels=3, embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size\n",
    "        )\n",
    "        \n",
    "        # Learnable position embeddings\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.n_patches + 1, embed_dim))\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        \n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        assert H == W == self.img_size, f\"Input image size ({H}*{W}) doesn't match model ({self.img_size}*{self.img_size})\"\n",
    "        \n",
    "        # Create patches\n",
    "        x = self.proj(x)  # (B, embed_dim, n_patches_h, n_patches_w)\n",
    "        x = x.flatten(2)  # (B, embed_dim, n_patches)\n",
    "        x = x.transpose(1, 2)  # (B, n_patches, embed_dim)\n",
    "        \n",
    "        # Add class token\n",
    "        cls_token = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        \n",
    "        # Add position embeddings\n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        \n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features * 4\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4., drop=0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = MLP(in_features=embed_dim, hidden_features=int(embed_dim * mlp_ratio))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=32, patch_size=4, in_channels=3, num_classes=100,\n",
    "                 embed_dim=256, depth=4, num_heads=4, mlp_ratio=4.):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_ratio)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.norm(x)\n",
    "        x = x[:, 0]  # Class token\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "# Training function\n",
    "def train_model(model, criterion, optimizer, num_epochs=20):\n",
    "    model.train()\n",
    "    total_step = len(train_loader)\n",
    "    train_times = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (i+1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        train_times.append(epoch_time)\n",
    "        print(f'Epoch {epoch+1} completed in {epoch_time:.2f} seconds')\n",
    "    \n",
    "    return train_times\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "        return accuracy\n",
    "\n",
    "# Model configurations to test\n",
    "configs = [\n",
    "    {'name': 'ViT-Tiny', 'patch_size': 4, 'embed_dim': 256, 'depth': 4, 'num_heads': 2, 'mlp_ratio': 2},\n",
    "    {'name': 'ViT-Small', 'patch_size': 8, 'embed_dim': 256, 'depth': 8, 'num_heads': 2, 'mlp_ratio': 2},\n",
    "    {'name': 'ViT-Medium', 'patch_size': 4, 'embed_dim': 512, 'depth': 4, 'num_heads': 4, 'mlp_ratio': 4},\n",
    "    {'name': 'ViT-Large', 'patch_size': 8, 'embed_dim': 512, 'depth': 8, 'num_heads': 4, 'mlp_ratio': 4},\n",
    "]\n",
    "results = []\n",
    "\n",
    "for config in configs:\n",
    "    print(f\"\\nTraining {config['name']} configuration...\")\n",
    "    print(f\"Patch size: {config['patch_size']}, Embed dim: {config['embed_dim']}, \"\n",
    "          f\"Depth: {config['depth']}, Heads: {config['num_heads']}, MLP ratio: {config['mlp_ratio']}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = VisionTransformer(\n",
    "        img_size=32,\n",
    "        patch_size=config['patch_size'],\n",
    "        embed_dim=config['embed_dim'],\n",
    "        depth=config['depth'],\n",
    "        num_heads=config['num_heads'],\n",
    "        mlp_ratio=config['mlp_ratio'],\n",
    "        num_classes=num_classes\n",
    "    ).to(device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Print model summary\n",
    "    print(\"\\nModel Summary:\")\n",
    "    summary(model, input_size=(batch_size, 3, 32, 32))\n",
    "    \n",
    "    # Train and evaluate\n",
    "    train_times = train_model(model, criterion, optimizer, num_epochs)\n",
    "    accuracy = evaluate_model(model)\n",
    "    \n",
    "    # Calculate parameters and FLOPs\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    flops = sum(p.numel() for p in model.parameters() if p.requires_grad) * 2 * 32 * 32  # Approximate\n",
    "    \n",
    "    results.append({\n",
    "        'name': config['name'],\n",
    "        'patch_size': config['patch_size'],\n",
    "        'embed_dim': config['embed_dim'],\n",
    "        'depth': config['depth'],\n",
    "        'num_heads': config['num_heads'],\n",
    "        'mlp_ratio': config['mlp_ratio'],\n",
    "        'params': total_params,\n",
    "        'flops': flops,\n",
    "        'avg_train_time': sum(train_times)/len(train_times),\n",
    "        'accuracy': accuracy\n",
    "    })\n",
    "\n",
    "# ResNet-18 baseline\n",
    "print(\"\\nTraining ResNet-18 baseline...\")\n",
    "resnet = torchvision.models.resnet18(num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(resnet.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\"\\nResNet-18 Summary:\")\n",
    "summary(resnet, input_size=(batch_size, 3, 32, 32))\n",
    "\n",
    "resnet_train_times = train_model(resnet, criterion, optimizer, 10)  # Train for 10 epochs\n",
    "resnet_accuracy = evaluate_model(resnet)\n",
    "\n",
    "resnet_params = sum(p.numel() for p in resnet.parameters())\n",
    "resnet_flops = sum(p.numel() for p in resnet.parameters() if p.requires_grad) * 2 * 32 * 32  # Approximate\n",
    "\n",
    "results.append({\n",
    "    'name': 'ResNet-18',\n",
    "    'patch_size': 'N/A',\n",
    "    'embed_dim': 'N/A',\n",
    "    'depth': 18,\n",
    "    'num_heads': 'N/A',\n",
    "    'mlp_ratio': 'N/A',\n",
    "    'params': resnet_params,\n",
    "    'flops': resnet_flops,\n",
    "    'avg_train_time': sum(resnet_train_times)/len(resnet_train_times),\n",
    "    'accuracy': resnet_accuracy\n",
    "})\n",
    "\n",
    "# Print results table\n",
    "print(\"\\nResults Summary:\")\n",
    "print(\"=\"*120)\n",
    "print(f\"{'Model':<15}{'Patch':<8}{'Embed':<8}{'Depth':<8}{'Heads':<8}{'MLP':<8}{'Params':<15}{'FLOPs':<15}{'Time/Epoch':<15}{'Accuracy':<10}\")\n",
    "print(\"-\"*120)\n",
    "for r in results:\n",
    "    print(f\"{r['name']:<15}{r['patch_size']:<8}{r['embed_dim']:<8}{r['depth']:<8}{r['num_heads']:<8}\"\n",
    "          f\"{r['mlp_ratio']:<8}{r['params']/1e6:.2f}M{'':<5}{r['flops']/1e9:.2f}G{'':<5}\"\n",
    "          f\"{r['avg_train_time']:.2f}s{'':<7}{r['accuracy']:.2f}%\")\n",
    "print(\"=\"*120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing tiny model ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([100]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([100, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch [1/5]: 100%|| 1563/1563 [01:57<00:00, 13.31it/s, loss=3.37]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 time: 117.42s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/5]: 100%|| 1563/1563 [01:53<00:00, 13.81it/s, loss=2.56]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 time: 113.22s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/5]: 100%|| 1563/1563 [01:52<00:00, 13.84it/s, loss=2.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 time: 112.93s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/5]: 100%|| 1563/1563 [01:52<00:00, 13.90it/s, loss=1.73]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 time: 112.42s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/5]: 100%|| 1563/1563 [01:54<00:00, 13.62it/s, loss=1.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 time: 114.79s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|| 313/313 [00:23<00:00, 13.53it/s]\n",
      "c:\\Users\\legon\\miniconda3\\envs\\RealTimeenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\legon\\.cache\\huggingface\\hub\\models--microsoft--swin-small-patch4-window7-224. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiny Test Accuracy: 66.60%\n",
      "tiny Avg Epoch Time: 114.15s\n",
      "\n",
      "=== Processing small model ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-small-patch4-window7-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([100, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([100]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch [1/5]: 100%|| 1563/1563 [02:42<00:00,  9.63it/s, loss=3.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 time: 162.35s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/5]: 100%|| 1563/1563 [02:41<00:00,  9.69it/s, loss=2.73]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 time: 161.34s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/5]: 100%|| 1563/1563 [02:36<00:00,  9.97it/s, loss=1.98]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 time: 156.80s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/5]: 100%|| 1563/1563 [02:34<00:00, 10.11it/s, loss=1.7] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 time: 154.65s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/5]: 100%|| 1563/1563 [02:33<00:00, 10.21it/s, loss=1.35] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 time: 153.11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|| 313/313 [00:30<00:00, 10.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small Test Accuracy: 70.36%\n",
      "small Avg Epoch Time: 157.65s\n",
      "\n",
      "=== Processing scratch model ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/5]: 100%|| 1563/1563 [04:04<00:00,  6.38it/s, loss=3.33]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 time: 244.95s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/5]: 100%|| 1563/1563 [04:04<00:00,  6.38it/s, loss=3.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 time: 244.99s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/5]: 100%|| 1563/1563 [04:05<00:00,  6.35it/s, loss=2.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 time: 246.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/5]: 100%|| 1563/1563 [04:01<00:00,  6.48it/s, loss=2.54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 time: 241.24s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/5]: 100%|| 1563/1563 [03:57<00:00,  6.58it/s, loss=2.07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 time: 237.52s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|| 313/313 [00:20<00:00, 14.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scratch Test Accuracy: 37.23%\n",
      "scratch Avg Epoch Time: 242.94s\n",
      "\n",
      "=== Results ===\n",
      "Model      | Accuracy (%) | Avg Epoch Time (s)\n",
      "----------------------------------------\n",
      "tiny       | 66.60        | 114.15            \n",
      "small      | 70.36        | 157.65            \n",
      "scratch    | 37.23        | 242.94            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''Problem 2 Fine-tuning pretrained Swin Transformer models from Hugging Face Transformers library on CIFAR-100 on\n",
    "Tiny (microsoft/swin-tiny-patch4-window7-224) and Small (microsoft/swin-small-patch4-window7-224) variants - on CIFAR-100\n",
    "'''\n",
    "# Hyperparameters\n",
    "num_epochs = 5\n",
    "batch_size = 32\n",
    "learning_rate = 2e-5\n",
    "image_size = 224  # Swin expects 224x224 input\n",
    "\n",
    "# Models to compare\n",
    "model_variants = {\n",
    "    'tiny': 'microsoft/swin-tiny-patch4-window7-224',\n",
    "    'small': 'microsoft/swin-small-patch4-window7-224',\n",
    "    'scratch': None\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Data preparation\n",
    "processor = AutoImageProcessor.from_pretrained(model_variants['tiny'])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=processor.image_mean, std=processor.image_std)\n",
    "])\n",
    "\n",
    "# CIFAR-100 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
    "                                           download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
    "                                          download=True, transform=transform)\n",
    "\n",
    "for model_name, model_path in model_variants.items():\n",
    "    print(f\"\\n=== Processing {model_name} model ===\")\n",
    "    \n",
    "    # Scratch model\n",
    "    if model_name == 'scratch':\n",
    "        config = SwinConfig(\n",
    "            image_size=image_size,\n",
    "            patch_size=4,\n",
    "            num_channels=3,\n",
    "            embed_dim=96,\n",
    "            depths=[2, 2, 6, 2],\n",
    "            num_heads=[3, 6, 12, 24],\n",
    "            window_size=7,\n",
    "            num_labels=100\n",
    "        )\n",
    "        model = SwinForImageClassification(config).to(device)\n",
    "    else:\n",
    "        model = SwinForImageClassification.from_pretrained(\n",
    "            model_path,\n",
    "            num_labels=100,  # CIFAR-100 has 100 classes\n",
    "            ignore_mismatched_sizes=True\n",
    "        ).to(device)\n",
    "        \n",
    "        # Freeze backbone parameters for pretrained models\n",
    "        for param in model.swin.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # Only train classifier head for pretrained models, all params for scratch\n",
    "    trainable_params = []\n",
    "    if model_name == 'scratch':\n",
    "        trainable_params = model.parameters()\n",
    "    else:\n",
    "        trainable_params = model.classifier.parameters()\n",
    "        for param in model.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    " \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(trainable_params, lr=learning_rate)\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    " \n",
    "    model.train()\n",
    "    epoch_times = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "        \n",
    "        for images, labels in progress_bar:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        epoch_times.append(epoch_time)\n",
    "        print(f\"Epoch {epoch+1} time: {epoch_time:.2f}s\")\n",
    "\n",
    "    # Testing\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc='Testing'):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images).logits\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    avg_epoch_time = sum(epoch_times) / len(epoch_times)\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'avg_epoch_time': avg_epoch_time\n",
    "    }\n",
    "    \n",
    "    print(f\"{model_name} Test Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"{model_name} Avg Epoch Time: {avg_epoch_time:.2f}s\")\n",
    "\n",
    "# Print results table\n",
    "print(\"\\n=== Results ===\")\n",
    "print(f\"{'Model':<10} | {'Accuracy (%)':<12} | {'Avg Epoch Time (s)':<18}\")\n",
    "print(\"-\" * 40)    \n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"{model_name:<10} | {metrics['accuracy']:<12.2f} | {metrics['avg_epoch_time']:<18.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RealTimeenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
