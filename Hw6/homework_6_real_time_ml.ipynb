{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\legon\\miniconda3\\envs\\RealTimeenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndevNumber = torch.cuda.current_device()\\ndevName = torch.cuda.get_device_name(devNumber)\\n\\nprint(f\"Current device number is: {devNumber}\")\\nprint(f\"GPU name is: {devName}\")'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import requests\n",
    "\n",
    "#including runtime measurements, accuracy metrics, and model size calculations (Homework 6)\n",
    "#For training the models with different layers and heads\n",
    "from itertools import product\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "import math\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "\n",
    "#Importing the Swin Transformer model from Hugging Face Transformers library for Problem 3\n",
    "import transformers \n",
    "from transformers import SwinForImageClassification, SwinConfig, AutoImageProcessor\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "#Check the GPU name and number\n",
    "'''\n",
    "devNumber = torch.cuda.current_device()\n",
    "devName = torch.cuda.get_device_name(devNumber)\n",
    "\n",
    "print(f\"Current device number is: {devNumber}\")\n",
    "print(f\"GPU name is: {devName}\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training ViT-Tiny configuration...\n",
      "Patch size: 4, Embed dim: 256, Depth: 4, Heads: 4, MLP ratio: 2\n",
      "\n",
      "Model Summary:\n",
      "Epoch [1/20], Step [100/782], Loss: 4.3374\n",
      "Epoch [1/20], Step [200/782], Loss: 4.2726\n",
      "Epoch [1/20], Step [300/782], Loss: 3.6670\n",
      "Epoch [1/20], Step [400/782], Loss: 3.7433\n",
      "Epoch [1/20], Step [500/782], Loss: 3.6169\n",
      "Epoch [1/20], Step [600/782], Loss: 3.7691\n",
      "Epoch [1/20], Step [700/782], Loss: 3.9920\n",
      "Epoch 1 completed in 19.00 seconds\n",
      "Epoch [2/20], Step [100/782], Loss: 3.5482\n",
      "Epoch [2/20], Step [200/782], Loss: 3.6156\n",
      "Epoch [2/20], Step [300/782], Loss: 3.6134\n",
      "Epoch [2/20], Step [400/782], Loss: 3.3801\n",
      "Epoch [2/20], Step [500/782], Loss: 3.9170\n",
      "Epoch [2/20], Step [600/782], Loss: 3.7673\n",
      "Epoch [2/20], Step [700/782], Loss: 3.5599\n",
      "Epoch 2 completed in 20.52 seconds\n",
      "Epoch [3/20], Step [100/782], Loss: 3.7843\n",
      "Epoch [3/20], Step [200/782], Loss: 3.4811\n",
      "Epoch [3/20], Step [300/782], Loss: 3.6207\n",
      "Epoch [3/20], Step [400/782], Loss: 3.3501\n",
      "Epoch [3/20], Step [500/782], Loss: 3.3048\n",
      "Epoch [3/20], Step [600/782], Loss: 3.0983\n",
      "Epoch [3/20], Step [700/782], Loss: 3.5164\n",
      "Epoch 3 completed in 17.33 seconds\n",
      "Epoch [4/20], Step [100/782], Loss: 3.3838\n",
      "Epoch [4/20], Step [200/782], Loss: 3.1551\n",
      "Epoch [4/20], Step [300/782], Loss: 3.1972\n",
      "Epoch [4/20], Step [400/782], Loss: 3.1864\n",
      "Epoch [4/20], Step [500/782], Loss: 3.4142\n",
      "Epoch [4/20], Step [600/782], Loss: 3.1344\n",
      "Epoch [4/20], Step [700/782], Loss: 3.0157\n",
      "Epoch 4 completed in 17.61 seconds\n",
      "Epoch [5/20], Step [100/782], Loss: 3.1742\n",
      "Epoch [5/20], Step [200/782], Loss: 3.2380\n",
      "Epoch [5/20], Step [300/782], Loss: 3.2464\n",
      "Epoch [5/20], Step [400/782], Loss: 3.4704\n",
      "Epoch [5/20], Step [500/782], Loss: 3.4966\n",
      "Epoch [5/20], Step [600/782], Loss: 3.1680\n",
      "Epoch [5/20], Step [700/782], Loss: 3.1775\n",
      "Epoch 5 completed in 19.19 seconds\n",
      "Epoch [6/20], Step [100/782], Loss: 3.0636\n",
      "Epoch [6/20], Step [200/782], Loss: 3.2951\n",
      "Epoch [6/20], Step [300/782], Loss: 3.1233\n",
      "Epoch [6/20], Step [400/782], Loss: 2.9502\n",
      "Epoch [6/20], Step [500/782], Loss: 3.5318\n",
      "Epoch [6/20], Step [600/782], Loss: 3.1739\n",
      "Epoch [6/20], Step [700/782], Loss: 2.9548\n",
      "Epoch 6 completed in 18.65 seconds\n",
      "Epoch [7/20], Step [100/782], Loss: 3.0597\n",
      "Epoch [7/20], Step [200/782], Loss: 3.2568\n",
      "Epoch [7/20], Step [300/782], Loss: 3.1651\n",
      "Epoch [7/20], Step [400/782], Loss: 3.2622\n",
      "Epoch [7/20], Step [500/782], Loss: 3.1057\n",
      "Epoch [7/20], Step [600/782], Loss: 3.0994\n",
      "Epoch [7/20], Step [700/782], Loss: 2.9119\n",
      "Epoch 7 completed in 18.82 seconds\n",
      "Epoch [8/20], Step [100/782], Loss: 3.1840\n",
      "Epoch [8/20], Step [200/782], Loss: 3.3886\n",
      "Epoch [8/20], Step [300/782], Loss: 2.7538\n",
      "Epoch [8/20], Step [400/782], Loss: 2.7378\n",
      "Epoch [8/20], Step [500/782], Loss: 3.3012\n",
      "Epoch [8/20], Step [600/782], Loss: 3.1292\n",
      "Epoch [8/20], Step [700/782], Loss: 3.2631\n",
      "Epoch 8 completed in 18.78 seconds\n",
      "Epoch [9/20], Step [100/782], Loss: 2.9237\n",
      "Epoch [9/20], Step [200/782], Loss: 3.2389\n",
      "Epoch [9/20], Step [300/782], Loss: 2.6558\n",
      "Epoch [9/20], Step [400/782], Loss: 2.9830\n",
      "Epoch [9/20], Step [500/782], Loss: 3.0467\n",
      "Epoch [9/20], Step [600/782], Loss: 2.7766\n",
      "Epoch [9/20], Step [700/782], Loss: 2.7758\n",
      "Epoch 9 completed in 19.12 seconds\n",
      "Epoch [10/20], Step [100/782], Loss: 2.8518\n",
      "Epoch [10/20], Step [200/782], Loss: 2.8852\n",
      "Epoch [10/20], Step [300/782], Loss: 2.8733\n",
      "Epoch [10/20], Step [400/782], Loss: 2.8881\n",
      "Epoch [10/20], Step [500/782], Loss: 2.9334\n",
      "Epoch [10/20], Step [600/782], Loss: 3.0832\n",
      "Epoch [10/20], Step [700/782], Loss: 3.3751\n",
      "Epoch 10 completed in 18.85 seconds\n",
      "Epoch [11/20], Step [100/782], Loss: 2.6185\n",
      "Epoch [11/20], Step [200/782], Loss: 2.9664\n",
      "Epoch [11/20], Step [300/782], Loss: 3.1030\n",
      "Epoch [11/20], Step [400/782], Loss: 2.7562\n",
      "Epoch [11/20], Step [500/782], Loss: 2.7904\n",
      "Epoch [11/20], Step [600/782], Loss: 3.0109\n",
      "Epoch [11/20], Step [700/782], Loss: 3.2685\n",
      "Epoch 11 completed in 19.84 seconds\n",
      "Epoch [12/20], Step [100/782], Loss: 2.8186\n",
      "Epoch [12/20], Step [200/782], Loss: 3.1849\n",
      "Epoch [12/20], Step [300/782], Loss: 2.5818\n",
      "Epoch [12/20], Step [400/782], Loss: 3.3915\n",
      "Epoch [12/20], Step [500/782], Loss: 2.9808\n",
      "Epoch [12/20], Step [600/782], Loss: 2.7168\n",
      "Epoch [12/20], Step [700/782], Loss: 2.6726\n",
      "Epoch 12 completed in 18.33 seconds\n",
      "Epoch [13/20], Step [100/782], Loss: 2.7724\n",
      "Epoch [13/20], Step [200/782], Loss: 2.7451\n",
      "Epoch [13/20], Step [300/782], Loss: 2.5918\n",
      "Epoch [13/20], Step [400/782], Loss: 2.8044\n",
      "Epoch [13/20], Step [500/782], Loss: 2.8719\n",
      "Epoch [13/20], Step [600/782], Loss: 2.9173\n",
      "Epoch [13/20], Step [700/782], Loss: 2.6074\n",
      "Epoch 13 completed in 20.10 seconds\n",
      "Epoch [14/20], Step [100/782], Loss: 2.4240\n",
      "Epoch [14/20], Step [200/782], Loss: 2.6044\n",
      "Epoch [14/20], Step [300/782], Loss: 2.7019\n",
      "Epoch [14/20], Step [400/782], Loss: 2.5168\n",
      "Epoch [14/20], Step [500/782], Loss: 2.7704\n",
      "Epoch [14/20], Step [600/782], Loss: 2.6128\n",
      "Epoch [14/20], Step [700/782], Loss: 2.8904\n",
      "Epoch 14 completed in 17.91 seconds\n",
      "Epoch [15/20], Step [100/782], Loss: 2.9575\n",
      "Epoch [15/20], Step [200/782], Loss: 2.7761\n",
      "Epoch [15/20], Step [300/782], Loss: 2.5997\n",
      "Epoch [15/20], Step [400/782], Loss: 2.7269\n",
      "Epoch [15/20], Step [500/782], Loss: 3.0538\n",
      "Epoch [15/20], Step [600/782], Loss: 2.3773\n",
      "Epoch [15/20], Step [700/782], Loss: 2.4800\n",
      "Epoch 15 completed in 19.37 seconds\n",
      "Epoch [16/20], Step [100/782], Loss: 2.8631\n",
      "Epoch [16/20], Step [200/782], Loss: 2.5725\n",
      "Epoch [16/20], Step [300/782], Loss: 2.9077\n",
      "Epoch [16/20], Step [400/782], Loss: 2.6691\n",
      "Epoch [16/20], Step [500/782], Loss: 2.5313\n",
      "Epoch [16/20], Step [600/782], Loss: 2.3874\n",
      "Epoch [16/20], Step [700/782], Loss: 2.6513\n",
      "Epoch 16 completed in 18.48 seconds\n",
      "Epoch [17/20], Step [100/782], Loss: 2.2722\n",
      "Epoch [17/20], Step [200/782], Loss: 2.5759\n",
      "Epoch [17/20], Step [300/782], Loss: 2.7034\n",
      "Epoch [17/20], Step [400/782], Loss: 2.6911\n",
      "Epoch [17/20], Step [500/782], Loss: 2.9093\n",
      "Epoch [17/20], Step [600/782], Loss: 2.9434\n",
      "Epoch [17/20], Step [700/782], Loss: 2.7967\n",
      "Epoch 17 completed in 18.65 seconds\n",
      "Epoch [18/20], Step [100/782], Loss: 2.7427\n",
      "Epoch [18/20], Step [200/782], Loss: 2.9017\n",
      "Epoch [18/20], Step [300/782], Loss: 2.5492\n",
      "Epoch [18/20], Step [400/782], Loss: 2.7791\n",
      "Epoch [18/20], Step [500/782], Loss: 2.7772\n",
      "Epoch [18/20], Step [600/782], Loss: 2.9973\n",
      "Epoch [18/20], Step [700/782], Loss: 2.4593\n",
      "Epoch 18 completed in 19.49 seconds\n",
      "Epoch [19/20], Step [100/782], Loss: 2.6312\n",
      "Epoch [19/20], Step [200/782], Loss: 2.4248\n",
      "Epoch [19/20], Step [300/782], Loss: 2.7709\n",
      "Epoch [19/20], Step [400/782], Loss: 2.6961\n",
      "Epoch [19/20], Step [500/782], Loss: 2.9157\n",
      "Epoch [19/20], Step [600/782], Loss: 2.8172\n",
      "Epoch [19/20], Step [700/782], Loss: 2.9352\n",
      "Epoch 19 completed in 19.08 seconds\n",
      "Epoch [20/20], Step [100/782], Loss: 2.6044\n",
      "Epoch [20/20], Step [200/782], Loss: 2.7676\n",
      "Epoch [20/20], Step [300/782], Loss: 2.8216\n",
      "Epoch [20/20], Step [400/782], Loss: 2.4581\n",
      "Epoch [20/20], Step [500/782], Loss: 2.3580\n",
      "Epoch [20/20], Step [600/782], Loss: 2.4434\n",
      "Epoch [20/20], Step [700/782], Loss: 2.5729\n",
      "Epoch 20 completed in 17.35 seconds\n",
      "Test Accuracy: 29.16%\n",
      "\n",
      "Training ViT-Small configuration...\n",
      "Patch size: 8, Embed dim: 256, Depth: 8, Heads: 4, MLP ratio: 2\n",
      "\n",
      "Model Summary:\n",
      "Epoch [1/20], Step [100/782], Loss: 4.3122\n",
      "Epoch [1/20], Step [200/782], Loss: 4.1018\n",
      "Epoch [1/20], Step [300/782], Loss: 4.1812\n",
      "Epoch [1/20], Step [400/782], Loss: 3.6105\n",
      "Epoch [1/20], Step [500/782], Loss: 4.1792\n",
      "Epoch [1/20], Step [600/782], Loss: 4.0845\n",
      "Epoch [1/20], Step [700/782], Loss: 4.3117\n",
      "Epoch 1 completed in 22.79 seconds\n",
      "Epoch [2/20], Step [100/782], Loss: 4.2619\n",
      "Epoch [2/20], Step [200/782], Loss: 4.0308\n",
      "Epoch [2/20], Step [300/782], Loss: 3.9866\n",
      "Epoch [2/20], Step [400/782], Loss: 4.0323\n",
      "Epoch [2/20], Step [500/782], Loss: 4.1302\n",
      "Epoch [2/20], Step [600/782], Loss: 3.9809\n",
      "Epoch [2/20], Step [700/782], Loss: 3.9868\n",
      "Epoch 2 completed in 22.59 seconds\n",
      "Epoch [3/20], Step [100/782], Loss: 3.9958\n",
      "Epoch [3/20], Step [200/782], Loss: 3.8757\n",
      "Epoch [3/20], Step [300/782], Loss: 4.1796\n",
      "Epoch [3/20], Step [400/782], Loss: 4.2701\n",
      "Epoch [3/20], Step [500/782], Loss: 4.1201\n",
      "Epoch [3/20], Step [600/782], Loss: 3.8470\n",
      "Epoch [3/20], Step [700/782], Loss: 4.0775\n",
      "Epoch 3 completed in 23.28 seconds\n",
      "Epoch [4/20], Step [100/782], Loss: 4.1543\n",
      "Epoch [4/20], Step [200/782], Loss: 3.9638\n",
      "Epoch [4/20], Step [300/782], Loss: 4.2086\n",
      "Epoch [4/20], Step [400/782], Loss: 3.8546\n",
      "Epoch [4/20], Step [500/782], Loss: 3.9117\n",
      "Epoch [4/20], Step [600/782], Loss: 4.0833\n",
      "Epoch [4/20], Step [700/782], Loss: 3.9497\n",
      "Epoch 4 completed in 22.62 seconds\n",
      "Epoch [5/20], Step [100/782], Loss: 3.8381\n",
      "Epoch [5/20], Step [200/782], Loss: 4.2097\n",
      "Epoch [5/20], Step [300/782], Loss: 3.9653\n",
      "Epoch [5/20], Step [400/782], Loss: 3.9529\n",
      "Epoch [5/20], Step [500/782], Loss: 3.8786\n",
      "Epoch [5/20], Step [600/782], Loss: 3.6614\n",
      "Epoch [5/20], Step [700/782], Loss: 4.0965\n",
      "Epoch 5 completed in 22.93 seconds\n",
      "Epoch [6/20], Step [100/782], Loss: 4.0958\n",
      "Epoch [6/20], Step [200/782], Loss: 3.7347\n",
      "Epoch [6/20], Step [300/782], Loss: 3.7313\n",
      "Epoch [6/20], Step [400/782], Loss: 3.9366\n",
      "Epoch [6/20], Step [500/782], Loss: 4.0799\n",
      "Epoch [6/20], Step [600/782], Loss: 3.8126\n",
      "Epoch [6/20], Step [700/782], Loss: 3.6120\n",
      "Epoch 6 completed in 22.88 seconds\n",
      "Epoch [7/20], Step [100/782], Loss: 4.1531\n",
      "Epoch [7/20], Step [200/782], Loss: 3.9889\n",
      "Epoch [7/20], Step [300/782], Loss: 4.4312\n",
      "Epoch [7/20], Step [400/782], Loss: 3.9162\n",
      "Epoch [7/20], Step [500/782], Loss: 4.1912\n",
      "Epoch [7/20], Step [600/782], Loss: 3.9645\n",
      "Epoch [7/20], Step [700/782], Loss: 3.9629\n",
      "Epoch 7 completed in 23.35 seconds\n",
      "Epoch [8/20], Step [100/782], Loss: 3.7179\n",
      "Epoch [8/20], Step [200/782], Loss: 3.8612\n",
      "Epoch [8/20], Step [300/782], Loss: 3.8304\n",
      "Epoch [8/20], Step [400/782], Loss: 3.9232\n",
      "Epoch [8/20], Step [500/782], Loss: 3.8218\n",
      "Epoch [8/20], Step [600/782], Loss: 3.6964\n",
      "Epoch [8/20], Step [700/782], Loss: 3.8025\n",
      "Epoch 8 completed in 23.58 seconds\n",
      "Epoch [9/20], Step [100/782], Loss: 3.7178\n",
      "Epoch [9/20], Step [200/782], Loss: 3.8168\n",
      "Epoch [9/20], Step [300/782], Loss: 3.8601\n",
      "Epoch [9/20], Step [400/782], Loss: 3.9889\n",
      "Epoch [9/20], Step [500/782], Loss: 3.7227\n",
      "Epoch [9/20], Step [600/782], Loss: 3.7216\n",
      "Epoch [9/20], Step [700/782], Loss: 3.4684\n",
      "Epoch 9 completed in 23.29 seconds\n",
      "Epoch [10/20], Step [100/782], Loss: 3.6341\n",
      "Epoch [10/20], Step [200/782], Loss: 3.9801\n",
      "Epoch [10/20], Step [300/782], Loss: 4.3464\n",
      "Epoch [10/20], Step [400/782], Loss: 4.0118\n",
      "Epoch [10/20], Step [500/782], Loss: 4.0004\n",
      "Epoch [10/20], Step [600/782], Loss: 3.8737\n",
      "Epoch [10/20], Step [700/782], Loss: 3.8576\n",
      "Epoch 10 completed in 22.76 seconds\n",
      "Epoch [11/20], Step [100/782], Loss: 3.7888\n",
      "Epoch [11/20], Step [200/782], Loss: 3.7210\n",
      "Epoch [11/20], Step [300/782], Loss: 3.8725\n",
      "Epoch [11/20], Step [400/782], Loss: 3.9353\n",
      "Epoch [11/20], Step [500/782], Loss: 3.6116\n",
      "Epoch [11/20], Step [600/782], Loss: 3.6528\n",
      "Epoch [11/20], Step [700/782], Loss: 3.8577\n",
      "Epoch 11 completed in 23.64 seconds\n",
      "Epoch [12/20], Step [100/782], Loss: 4.1212\n",
      "Epoch [12/20], Step [200/782], Loss: 4.0249\n",
      "Epoch [12/20], Step [300/782], Loss: 3.9727\n",
      "Epoch [12/20], Step [400/782], Loss: 3.8237\n",
      "Epoch [12/20], Step [500/782], Loss: 3.6651\n",
      "Epoch [12/20], Step [600/782], Loss: 3.5421\n",
      "Epoch [12/20], Step [700/782], Loss: 3.7492\n",
      "Epoch 12 completed in 24.80 seconds\n",
      "Epoch [13/20], Step [100/782], Loss: 3.5051\n",
      "Epoch [13/20], Step [200/782], Loss: 3.8441\n",
      "Epoch [13/20], Step [300/782], Loss: 3.8844\n",
      "Epoch [13/20], Step [400/782], Loss: 3.8845\n",
      "Epoch [13/20], Step [500/782], Loss: 3.5493\n",
      "Epoch [13/20], Step [600/782], Loss: 3.9744\n",
      "Epoch [13/20], Step [700/782], Loss: 3.6876\n",
      "Epoch 13 completed in 23.30 seconds\n",
      "Epoch [14/20], Step [100/782], Loss: 3.8633\n",
      "Epoch [14/20], Step [200/782], Loss: 3.5296\n",
      "Epoch [14/20], Step [300/782], Loss: 3.6944\n",
      "Epoch [14/20], Step [400/782], Loss: 3.7182\n",
      "Epoch [14/20], Step [500/782], Loss: 3.7266\n",
      "Epoch [14/20], Step [600/782], Loss: 3.5962\n",
      "Epoch [14/20], Step [700/782], Loss: 3.9066\n",
      "Epoch 14 completed in 24.39 seconds\n",
      "Epoch [15/20], Step [100/782], Loss: 3.3148\n",
      "Epoch [15/20], Step [200/782], Loss: 3.5881\n",
      "Epoch [15/20], Step [300/782], Loss: 3.6963\n",
      "Epoch [15/20], Step [400/782], Loss: 3.5016\n",
      "Epoch [15/20], Step [500/782], Loss: 3.7486\n",
      "Epoch [15/20], Step [600/782], Loss: 3.5849\n",
      "Epoch [15/20], Step [700/782], Loss: 3.5128\n",
      "Epoch 15 completed in 23.61 seconds\n",
      "Epoch [16/20], Step [100/782], Loss: 3.8095\n",
      "Epoch [16/20], Step [200/782], Loss: 3.7690\n",
      "Epoch [16/20], Step [300/782], Loss: 3.6341\n",
      "Epoch [16/20], Step [400/782], Loss: 3.9743\n",
      "Epoch [16/20], Step [500/782], Loss: 3.7070\n",
      "Epoch [16/20], Step [600/782], Loss: 3.6796\n",
      "Epoch [16/20], Step [700/782], Loss: 3.9215\n",
      "Epoch 16 completed in 22.96 seconds\n",
      "Epoch [17/20], Step [100/782], Loss: 3.7451\n",
      "Epoch [17/20], Step [200/782], Loss: 3.4771\n",
      "Epoch [17/20], Step [300/782], Loss: 4.0667\n",
      "Epoch [17/20], Step [400/782], Loss: 3.8063\n",
      "Epoch [17/20], Step [500/782], Loss: 3.6624\n",
      "Epoch [17/20], Step [600/782], Loss: 3.9138\n",
      "Epoch [17/20], Step [700/782], Loss: 3.9286\n",
      "Epoch 17 completed in 22.55 seconds\n",
      "Epoch [18/20], Step [100/782], Loss: 3.5972\n",
      "Epoch [18/20], Step [200/782], Loss: 3.8960\n",
      "Epoch [18/20], Step [300/782], Loss: 3.6153\n",
      "Epoch [18/20], Step [400/782], Loss: 3.8358\n",
      "Epoch [18/20], Step [500/782], Loss: 3.9528\n",
      "Epoch [18/20], Step [600/782], Loss: 3.6046\n",
      "Epoch [18/20], Step [700/782], Loss: 4.0077\n",
      "Epoch 18 completed in 21.74 seconds\n",
      "Epoch [19/20], Step [100/782], Loss: 3.6452\n",
      "Epoch [19/20], Step [200/782], Loss: 3.9293\n",
      "Epoch [19/20], Step [300/782], Loss: 3.9189\n",
      "Epoch [19/20], Step [400/782], Loss: 3.5707\n",
      "Epoch [19/20], Step [500/782], Loss: 3.8323\n",
      "Epoch [19/20], Step [600/782], Loss: 4.0688\n",
      "Epoch [19/20], Step [700/782], Loss: 3.9991\n",
      "Epoch 19 completed in 21.55 seconds\n",
      "Epoch [20/20], Step [100/782], Loss: 3.4902\n",
      "Epoch [20/20], Step [200/782], Loss: 3.7097\n",
      "Epoch [20/20], Step [300/782], Loss: 3.5804\n",
      "Epoch [20/20], Step [400/782], Loss: 3.7824\n",
      "Epoch [20/20], Step [500/782], Loss: 3.8516\n",
      "Epoch [20/20], Step [600/782], Loss: 3.6952\n",
      "Epoch [20/20], Step [700/782], Loss: 4.0142\n",
      "Epoch 20 completed in 23.35 seconds\n",
      "Test Accuracy: 8.85%\n",
      "\n",
      "Training ViT-Medium configuration...\n",
      "Patch size: 4, Embed dim: 512, Depth: 4, Heads: 8, MLP ratio: 4\n",
      "\n",
      "Model Summary:\n",
      "Epoch [1/20], Step [100/782], Loss: 4.2115\n",
      "Epoch [1/20], Step [200/782], Loss: 4.2512\n",
      "Epoch [1/20], Step [300/782], Loss: 4.1353\n",
      "Epoch [1/20], Step [400/782], Loss: 4.2026\n",
      "Epoch [1/20], Step [500/782], Loss: 3.8784\n",
      "Epoch [1/20], Step [600/782], Loss: 4.0325\n",
      "Epoch [1/20], Step [700/782], Loss: 4.0098\n",
      "Epoch 1 completed in 22.47 seconds\n",
      "Epoch [2/20], Step [100/782], Loss: 3.8723\n",
      "Epoch [2/20], Step [200/782], Loss: 3.8487\n",
      "Epoch [2/20], Step [300/782], Loss: 3.8709\n",
      "Epoch [2/20], Step [400/782], Loss: 3.8774\n",
      "Epoch [2/20], Step [500/782], Loss: 3.5638\n",
      "Epoch [2/20], Step [600/782], Loss: 3.5980\n",
      "Epoch [2/20], Step [700/782], Loss: 3.9814\n",
      "Epoch 2 completed in 23.25 seconds\n",
      "Epoch [3/20], Step [100/782], Loss: 3.9252\n",
      "Epoch [3/20], Step [200/782], Loss: 3.7802\n",
      "Epoch [3/20], Step [300/782], Loss: 3.8697\n",
      "Epoch [3/20], Step [400/782], Loss: 3.6693\n",
      "Epoch [3/20], Step [500/782], Loss: 3.9382\n",
      "Epoch [3/20], Step [600/782], Loss: 3.8859\n",
      "Epoch [3/20], Step [700/782], Loss: 3.7078\n",
      "Epoch 3 completed in 22.67 seconds\n",
      "Epoch [4/20], Step [100/782], Loss: 3.3413\n",
      "Epoch [4/20], Step [200/782], Loss: 3.8602\n",
      "Epoch [4/20], Step [300/782], Loss: 3.6019\n",
      "Epoch [4/20], Step [400/782], Loss: 3.8491\n",
      "Epoch [4/20], Step [500/782], Loss: 3.4953\n",
      "Epoch [4/20], Step [600/782], Loss: 3.9459\n",
      "Epoch [4/20], Step [700/782], Loss: 3.7505\n",
      "Epoch 4 completed in 23.32 seconds\n",
      "Epoch [5/20], Step [100/782], Loss: 3.8196\n",
      "Epoch [5/20], Step [200/782], Loss: 3.7842\n",
      "Epoch [5/20], Step [300/782], Loss: 3.4584\n",
      "Epoch [5/20], Step [400/782], Loss: 3.7840\n",
      "Epoch [5/20], Step [500/782], Loss: 3.8252\n",
      "Epoch [5/20], Step [600/782], Loss: 3.6894\n",
      "Epoch [5/20], Step [700/782], Loss: 3.6057\n",
      "Epoch 5 completed in 24.19 seconds\n",
      "Epoch [6/20], Step [100/782], Loss: 4.0454\n",
      "Epoch [6/20], Step [200/782], Loss: 3.2227\n",
      "Epoch [6/20], Step [300/782], Loss: 4.1118\n",
      "Epoch [6/20], Step [400/782], Loss: 3.7769\n",
      "Epoch [6/20], Step [500/782], Loss: 3.7503\n",
      "Epoch [6/20], Step [600/782], Loss: 4.0304\n",
      "Epoch [6/20], Step [700/782], Loss: 4.0290\n",
      "Epoch 6 completed in 22.84 seconds\n",
      "Epoch [7/20], Step [100/782], Loss: 3.6921\n",
      "Epoch [7/20], Step [200/782], Loss: 3.6769\n",
      "Epoch [7/20], Step [300/782], Loss: 3.7920\n",
      "Epoch [7/20], Step [400/782], Loss: 4.1354\n",
      "Epoch [7/20], Step [500/782], Loss: 3.6144\n",
      "Epoch [7/20], Step [600/782], Loss: 3.9767\n",
      "Epoch [7/20], Step [700/782], Loss: 3.9316\n",
      "Epoch 7 completed in 22.81 seconds\n",
      "Epoch [8/20], Step [100/782], Loss: 3.5425\n",
      "Epoch [8/20], Step [200/782], Loss: 3.8398\n",
      "Epoch [8/20], Step [300/782], Loss: 3.6945\n",
      "Epoch [8/20], Step [400/782], Loss: 3.7724\n",
      "Epoch [8/20], Step [500/782], Loss: 3.9309\n",
      "Epoch [8/20], Step [600/782], Loss: 3.7777\n",
      "Epoch [8/20], Step [700/782], Loss: 3.8065\n",
      "Epoch 8 completed in 22.99 seconds\n",
      "Epoch [9/20], Step [100/782], Loss: 3.9715\n",
      "Epoch [9/20], Step [200/782], Loss: 3.8728\n",
      "Epoch [9/20], Step [300/782], Loss: 3.9561\n",
      "Epoch [9/20], Step [400/782], Loss: 3.8710\n",
      "Epoch [9/20], Step [500/782], Loss: 3.7784\n",
      "Epoch [9/20], Step [600/782], Loss: 3.9142\n",
      "Epoch [9/20], Step [700/782], Loss: 3.8486\n",
      "Epoch 9 completed in 23.01 seconds\n",
      "Epoch [10/20], Step [100/782], Loss: 3.8304\n",
      "Epoch [10/20], Step [200/782], Loss: 3.6638\n",
      "Epoch [10/20], Step [300/782], Loss: 3.5226\n",
      "Epoch [10/20], Step [400/782], Loss: 3.8382\n",
      "Epoch [10/20], Step [500/782], Loss: 3.6753\n",
      "Epoch [10/20], Step [600/782], Loss: 3.6096\n",
      "Epoch [10/20], Step [700/782], Loss: 3.7765\n",
      "Epoch 10 completed in 22.88 seconds\n",
      "Epoch [11/20], Step [100/782], Loss: 3.7381\n",
      "Epoch [11/20], Step [200/782], Loss: 3.8850\n",
      "Epoch [11/20], Step [300/782], Loss: 4.0252\n",
      "Epoch [11/20], Step [400/782], Loss: 3.6369\n",
      "Epoch [11/20], Step [500/782], Loss: 3.6548\n",
      "Epoch [11/20], Step [600/782], Loss: 3.8475\n",
      "Epoch [11/20], Step [700/782], Loss: 3.6397\n",
      "Epoch 11 completed in 22.27 seconds\n",
      "Epoch [12/20], Step [100/782], Loss: 3.5543\n",
      "Epoch [12/20], Step [200/782], Loss: 3.6883\n",
      "Epoch [12/20], Step [300/782], Loss: 3.5961\n",
      "Epoch [12/20], Step [400/782], Loss: 3.7609\n",
      "Epoch [12/20], Step [500/782], Loss: 3.9526\n",
      "Epoch [12/20], Step [600/782], Loss: 3.8312\n",
      "Epoch [12/20], Step [700/782], Loss: 3.6846\n",
      "Epoch 12 completed in 22.22 seconds\n",
      "Epoch [13/20], Step [100/782], Loss: 3.7134\n",
      "Epoch [13/20], Step [200/782], Loss: 3.8190\n",
      "Epoch [13/20], Step [300/782], Loss: 3.8899\n",
      "Epoch [13/20], Step [400/782], Loss: 3.8128\n",
      "Epoch [13/20], Step [500/782], Loss: 3.8615\n",
      "Epoch [13/20], Step [600/782], Loss: 3.4650\n",
      "Epoch [13/20], Step [700/782], Loss: 3.9273\n",
      "Epoch 13 completed in 22.42 seconds\n",
      "Epoch [14/20], Step [100/782], Loss: 3.5757\n",
      "Epoch [14/20], Step [200/782], Loss: 3.5410\n",
      "Epoch [14/20], Step [300/782], Loss: 4.0238\n",
      "Epoch [14/20], Step [400/782], Loss: 3.9313\n",
      "Epoch [14/20], Step [500/782], Loss: 3.8581\n",
      "Epoch [14/20], Step [600/782], Loss: 3.6535\n",
      "Epoch [14/20], Step [700/782], Loss: 3.9071\n",
      "Epoch 14 completed in 22.28 seconds\n",
      "Epoch [15/20], Step [100/782], Loss: 3.9324\n",
      "Epoch [15/20], Step [200/782], Loss: 3.6347\n",
      "Epoch [15/20], Step [300/782], Loss: 3.8233\n",
      "Epoch [15/20], Step [400/782], Loss: 3.9114\n",
      "Epoch [15/20], Step [500/782], Loss: 3.7974\n",
      "Epoch [15/20], Step [600/782], Loss: 3.8057\n",
      "Epoch [15/20], Step [700/782], Loss: 3.6520\n",
      "Epoch 15 completed in 22.43 seconds\n",
      "Epoch [16/20], Step [100/782], Loss: 3.6708\n",
      "Epoch [16/20], Step [200/782], Loss: 3.6219\n",
      "Epoch [16/20], Step [300/782], Loss: 3.8017\n",
      "Epoch [16/20], Step [400/782], Loss: 3.8182\n",
      "Epoch [16/20], Step [500/782], Loss: 3.6611\n",
      "Epoch [16/20], Step [600/782], Loss: 3.7743\n",
      "Epoch [16/20], Step [700/782], Loss: 3.8082\n",
      "Epoch 16 completed in 23.17 seconds\n",
      "Epoch [17/20], Step [100/782], Loss: 3.7718\n",
      "Epoch [17/20], Step [200/782], Loss: 3.8261\n",
      "Epoch [17/20], Step [300/782], Loss: 3.4437\n",
      "Epoch [17/20], Step [400/782], Loss: 3.5598\n",
      "Epoch [17/20], Step [500/782], Loss: 3.8916\n",
      "Epoch [17/20], Step [600/782], Loss: 3.5724\n",
      "Epoch [17/20], Step [700/782], Loss: 3.8572\n",
      "Epoch 17 completed in 23.11 seconds\n",
      "Epoch [18/20], Step [100/782], Loss: 3.5168\n",
      "Epoch [18/20], Step [200/782], Loss: 3.8255\n",
      "Epoch [18/20], Step [300/782], Loss: 3.7167\n",
      "Epoch [18/20], Step [400/782], Loss: 3.6083\n",
      "Epoch [18/20], Step [500/782], Loss: 3.6419\n",
      "Epoch [18/20], Step [600/782], Loss: 3.6747\n",
      "Epoch [18/20], Step [700/782], Loss: 3.5080\n",
      "Epoch 18 completed in 22.79 seconds\n",
      "Epoch [19/20], Step [100/782], Loss: 3.8014\n",
      "Epoch [19/20], Step [200/782], Loss: 3.6419\n",
      "Epoch [19/20], Step [300/782], Loss: 3.6579\n",
      "Epoch [19/20], Step [400/782], Loss: 4.2176\n",
      "Epoch [19/20], Step [500/782], Loss: 3.8151\n",
      "Epoch [19/20], Step [600/782], Loss: 3.6851\n",
      "Epoch [19/20], Step [700/782], Loss: 3.7221\n",
      "Epoch 19 completed in 21.89 seconds\n",
      "Epoch [20/20], Step [100/782], Loss: 3.6782\n",
      "Epoch [20/20], Step [200/782], Loss: 3.8419\n",
      "Epoch [20/20], Step [300/782], Loss: 4.1186\n",
      "Epoch [20/20], Step [400/782], Loss: 3.7887\n",
      "Epoch [20/20], Step [500/782], Loss: 3.6987\n",
      "Epoch [20/20], Step [600/782], Loss: 3.8263\n",
      "Epoch [20/20], Step [700/782], Loss: 3.5000\n",
      "Epoch 20 completed in 22.17 seconds\n",
      "Test Accuracy: 12.60%\n",
      "\n",
      "Training ViT-Large configuration...\n",
      "Patch size: 8, Embed dim: 512, Depth: 8, Heads: 8, MLP ratio: 4\n",
      "\n",
      "Model Summary:\n",
      "Epoch [1/20], Step [100/782], Loss: 4.5922\n",
      "Epoch [1/20], Step [200/782], Loss: 4.3295\n",
      "Epoch [1/20], Step [300/782], Loss: 4.3835\n",
      "Epoch [1/20], Step [400/782], Loss: 4.3326\n",
      "Epoch [1/20], Step [500/782], Loss: 4.3347\n",
      "Epoch [1/20], Step [600/782], Loss: 4.3657\n",
      "Epoch [1/20], Step [700/782], Loss: 4.1617\n",
      "Epoch 1 completed in 23.82 seconds\n",
      "Epoch [2/20], Step [100/782], Loss: 3.9362\n",
      "Epoch [2/20], Step [200/782], Loss: 4.1079\n",
      "Epoch [2/20], Step [300/782], Loss: 3.9893\n",
      "Epoch [2/20], Step [400/782], Loss: 4.0437\n",
      "Epoch [2/20], Step [500/782], Loss: 4.2380\n",
      "Epoch [2/20], Step [600/782], Loss: 3.9875\n",
      "Epoch [2/20], Step [700/782], Loss: 4.1366\n",
      "Epoch 2 completed in 23.20 seconds\n",
      "Epoch [3/20], Step [100/782], Loss: 4.0770\n",
      "Epoch [3/20], Step [200/782], Loss: 4.0824\n",
      "Epoch [3/20], Step [300/782], Loss: 4.0899\n",
      "Epoch [3/20], Step [400/782], Loss: 4.0790\n",
      "Epoch [3/20], Step [500/782], Loss: 4.1425\n",
      "Epoch [3/20], Step [600/782], Loss: 4.1346\n",
      "Epoch [3/20], Step [700/782], Loss: 4.0929\n",
      "Epoch 3 completed in 24.07 seconds\n",
      "Epoch [4/20], Step [100/782], Loss: 4.1083\n",
      "Epoch [4/20], Step [200/782], Loss: 4.1058\n",
      "Epoch [4/20], Step [300/782], Loss: 4.0272\n",
      "Epoch [4/20], Step [400/782], Loss: 3.9479\n",
      "Epoch [4/20], Step [500/782], Loss: 3.8131\n",
      "Epoch [4/20], Step [600/782], Loss: 3.8034\n",
      "Epoch [4/20], Step [700/782], Loss: 4.2008\n",
      "Epoch 4 completed in 23.07 seconds\n",
      "Epoch [5/20], Step [100/782], Loss: 4.2008\n",
      "Epoch [5/20], Step [200/782], Loss: 4.1314\n",
      "Epoch [5/20], Step [300/782], Loss: 3.9923\n",
      "Epoch [5/20], Step [400/782], Loss: 4.0540\n",
      "Epoch [5/20], Step [500/782], Loss: 4.2832\n",
      "Epoch [5/20], Step [600/782], Loss: 3.8361\n",
      "Epoch [5/20], Step [700/782], Loss: 4.0375\n",
      "Epoch 5 completed in 23.60 seconds\n",
      "Epoch [6/20], Step [100/782], Loss: 4.2407\n",
      "Epoch [6/20], Step [200/782], Loss: 4.0657\n",
      "Epoch [6/20], Step [300/782], Loss: 3.9676\n",
      "Epoch [6/20], Step [400/782], Loss: 4.0108\n",
      "Epoch [6/20], Step [500/782], Loss: 4.1210\n",
      "Epoch [6/20], Step [600/782], Loss: 3.9422\n",
      "Epoch [6/20], Step [700/782], Loss: 4.0697\n",
      "Epoch 6 completed in 22.82 seconds\n",
      "Epoch [7/20], Step [100/782], Loss: 4.0110\n",
      "Epoch [7/20], Step [200/782], Loss: 4.1177\n",
      "Epoch [7/20], Step [300/782], Loss: 3.8282\n",
      "Epoch [7/20], Step [400/782], Loss: 3.9834\n",
      "Epoch [7/20], Step [500/782], Loss: 4.0077\n",
      "Epoch [7/20], Step [600/782], Loss: 4.1730\n",
      "Epoch [7/20], Step [700/782], Loss: 4.1769\n",
      "Epoch 7 completed in 23.32 seconds\n",
      "Epoch [8/20], Step [100/782], Loss: 4.1698\n",
      "Epoch [8/20], Step [200/782], Loss: 4.0591\n",
      "Epoch [8/20], Step [300/782], Loss: 4.1488\n",
      "Epoch [8/20], Step [400/782], Loss: 4.1413\n",
      "Epoch [8/20], Step [500/782], Loss: 4.0348\n",
      "Epoch [8/20], Step [600/782], Loss: 4.1038\n",
      "Epoch [8/20], Step [700/782], Loss: 4.3700\n",
      "Epoch 8 completed in 23.36 seconds\n",
      "Epoch [9/20], Step [100/782], Loss: 4.1040\n",
      "Epoch [9/20], Step [200/782], Loss: 3.7895\n",
      "Epoch [9/20], Step [300/782], Loss: 3.9576\n",
      "Epoch [9/20], Step [400/782], Loss: 4.0278\n",
      "Epoch [9/20], Step [500/782], Loss: 3.8561\n",
      "Epoch [9/20], Step [600/782], Loss: 4.1565\n",
      "Epoch [9/20], Step [700/782], Loss: 4.0890\n",
      "Epoch 9 completed in 23.69 seconds\n",
      "Epoch [10/20], Step [100/782], Loss: 3.7623\n",
      "Epoch [10/20], Step [200/782], Loss: 4.1569\n",
      "Epoch [10/20], Step [300/782], Loss: 3.7876\n",
      "Epoch [10/20], Step [400/782], Loss: 3.8379\n",
      "Epoch [10/20], Step [500/782], Loss: 4.2133\n",
      "Epoch [10/20], Step [600/782], Loss: 4.1089\n",
      "Epoch [10/20], Step [700/782], Loss: 4.2052\n",
      "Epoch 10 completed in 23.39 seconds\n",
      "Epoch [11/20], Step [100/782], Loss: 3.8766\n",
      "Epoch [11/20], Step [200/782], Loss: 3.9958\n",
      "Epoch [11/20], Step [300/782], Loss: 4.1627\n",
      "Epoch [11/20], Step [400/782], Loss: 4.1318\n",
      "Epoch [11/20], Step [500/782], Loss: 4.1740\n",
      "Epoch [11/20], Step [600/782], Loss: 3.9274\n",
      "Epoch [11/20], Step [700/782], Loss: 3.8927\n",
      "Epoch 11 completed in 23.46 seconds\n",
      "Epoch [12/20], Step [100/782], Loss: 4.0607\n",
      "Epoch [12/20], Step [200/782], Loss: 3.9975\n",
      "Epoch [12/20], Step [300/782], Loss: 3.7218\n",
      "Epoch [12/20], Step [400/782], Loss: 4.0599\n",
      "Epoch [12/20], Step [500/782], Loss: 3.7820\n",
      "Epoch [12/20], Step [600/782], Loss: 4.2620\n",
      "Epoch [12/20], Step [700/782], Loss: 3.9231\n",
      "Epoch 12 completed in 22.31 seconds\n",
      "Epoch [13/20], Step [100/782], Loss: 3.8342\n",
      "Epoch [13/20], Step [200/782], Loss: 3.9302\n",
      "Epoch [13/20], Step [300/782], Loss: 3.7895\n",
      "Epoch [13/20], Step [400/782], Loss: 3.6644\n",
      "Epoch [13/20], Step [500/782], Loss: 4.0083\n",
      "Epoch [13/20], Step [600/782], Loss: 3.9472\n",
      "Epoch [13/20], Step [700/782], Loss: 4.2660\n",
      "Epoch 13 completed in 24.47 seconds\n",
      "Epoch [14/20], Step [100/782], Loss: 4.0849\n",
      "Epoch [14/20], Step [200/782], Loss: 4.1296\n",
      "Epoch [14/20], Step [300/782], Loss: 3.8193\n",
      "Epoch [14/20], Step [400/782], Loss: 4.1591\n",
      "Epoch [14/20], Step [500/782], Loss: 4.0424\n",
      "Epoch [14/20], Step [600/782], Loss: 4.0990\n",
      "Epoch [14/20], Step [700/782], Loss: 4.0372\n",
      "Epoch 14 completed in 23.64 seconds\n",
      "Epoch [15/20], Step [100/782], Loss: 4.2499\n",
      "Epoch [15/20], Step [200/782], Loss: 3.8417\n",
      "Epoch [15/20], Step [300/782], Loss: 4.0282\n",
      "Epoch [15/20], Step [400/782], Loss: 3.8595\n",
      "Epoch [15/20], Step [500/782], Loss: 4.2499\n",
      "Epoch [15/20], Step [600/782], Loss: 3.9849\n",
      "Epoch [15/20], Step [700/782], Loss: 3.9916\n",
      "Epoch 15 completed in 23.58 seconds\n",
      "Epoch [16/20], Step [100/782], Loss: 3.7965\n",
      "Epoch [16/20], Step [200/782], Loss: 4.0806\n",
      "Epoch [16/20], Step [300/782], Loss: 4.2035\n",
      "Epoch [16/20], Step [400/782], Loss: 4.0285\n",
      "Epoch [16/20], Step [500/782], Loss: 3.9035\n",
      "Epoch [16/20], Step [600/782], Loss: 4.1783\n",
      "Epoch [16/20], Step [700/782], Loss: 3.9028\n",
      "Epoch 16 completed in 23.93 seconds\n",
      "Epoch [17/20], Step [100/782], Loss: 4.0420\n",
      "Epoch [17/20], Step [200/782], Loss: 4.0509\n",
      "Epoch [17/20], Step [300/782], Loss: 4.1892\n",
      "Epoch [17/20], Step [400/782], Loss: 3.7589\n",
      "Epoch [17/20], Step [500/782], Loss: 4.1468\n",
      "Epoch [17/20], Step [600/782], Loss: 4.0550\n",
      "Epoch [17/20], Step [700/782], Loss: 4.0414\n",
      "Epoch 17 completed in 23.81 seconds\n",
      "Epoch [18/20], Step [100/782], Loss: 4.1476\n",
      "Epoch [18/20], Step [200/782], Loss: 3.7916\n",
      "Epoch [18/20], Step [300/782], Loss: 3.9761\n",
      "Epoch [18/20], Step [400/782], Loss: 3.9070\n",
      "Epoch [18/20], Step [500/782], Loss: 3.9320\n",
      "Epoch [18/20], Step [600/782], Loss: 3.9227\n",
      "Epoch [18/20], Step [700/782], Loss: 3.6293\n",
      "Epoch 18 completed in 23.65 seconds\n",
      "Epoch [19/20], Step [100/782], Loss: 4.0354\n",
      "Epoch [19/20], Step [200/782], Loss: 3.8101\n",
      "Epoch [19/20], Step [300/782], Loss: 3.9415\n",
      "Epoch [19/20], Step [400/782], Loss: 3.8582\n",
      "Epoch [19/20], Step [500/782], Loss: 3.9969\n",
      "Epoch [19/20], Step [600/782], Loss: 4.1863\n",
      "Epoch [19/20], Step [700/782], Loss: 4.1079\n",
      "Epoch 19 completed in 23.36 seconds\n",
      "Epoch [20/20], Step [100/782], Loss: 4.0787\n",
      "Epoch [20/20], Step [200/782], Loss: 4.0692\n",
      "Epoch [20/20], Step [300/782], Loss: 4.0457\n",
      "Epoch [20/20], Step [400/782], Loss: 4.0093\n",
      "Epoch [20/20], Step [500/782], Loss: 3.7561\n",
      "Epoch [20/20], Step [600/782], Loss: 4.0700\n",
      "Epoch [20/20], Step [700/782], Loss: 3.8361\n",
      "Epoch 20 completed in 23.71 seconds\n",
      "Test Accuracy: 8.39%\n",
      "\n",
      "Training ResNet-18 baseline...\n",
      "\n",
      "ResNet-18 Summary:\n",
      "Epoch [1/20], Step [100/782], Loss: 3.8772\n",
      "Epoch [1/20], Step [200/782], Loss: 3.8633\n",
      "Epoch [1/20], Step [300/782], Loss: 3.4256\n",
      "Epoch [1/20], Step [400/782], Loss: 3.4897\n",
      "Epoch [1/20], Step [500/782], Loss: 3.5169\n",
      "Epoch [1/20], Step [600/782], Loss: 3.2451\n",
      "Epoch [1/20], Step [700/782], Loss: 3.3523\n",
      "Epoch 1 completed in 16.33 seconds\n",
      "Epoch [2/20], Step [100/782], Loss: 3.3196\n",
      "Epoch [2/20], Step [200/782], Loss: 2.8591\n",
      "Epoch [2/20], Step [300/782], Loss: 2.9878\n",
      "Epoch [2/20], Step [400/782], Loss: 2.7011\n",
      "Epoch [2/20], Step [500/782], Loss: 3.2972\n",
      "Epoch [2/20], Step [600/782], Loss: 2.7554\n",
      "Epoch [2/20], Step [700/782], Loss: 2.4572\n",
      "Epoch 2 completed in 16.67 seconds\n",
      "Epoch [3/20], Step [100/782], Loss: 2.4132\n",
      "Epoch [3/20], Step [200/782], Loss: 2.1114\n",
      "Epoch [3/20], Step [300/782], Loss: 2.9040\n",
      "Epoch [3/20], Step [400/782], Loss: 2.3969\n",
      "Epoch [3/20], Step [500/782], Loss: 2.2127\n",
      "Epoch [3/20], Step [600/782], Loss: 2.3463\n",
      "Epoch [3/20], Step [700/782], Loss: 2.4644\n",
      "Epoch 3 completed in 16.37 seconds\n",
      "Epoch [4/20], Step [100/782], Loss: 1.8814\n",
      "Epoch [4/20], Step [200/782], Loss: 2.0155\n",
      "Epoch [4/20], Step [300/782], Loss: 1.7640\n",
      "Epoch [4/20], Step [400/782], Loss: 2.0412\n",
      "Epoch [4/20], Step [500/782], Loss: 2.1025\n",
      "Epoch [4/20], Step [600/782], Loss: 1.9935\n",
      "Epoch [4/20], Step [700/782], Loss: 2.2706\n",
      "Epoch 4 completed in 16.36 seconds\n",
      "Epoch [5/20], Step [100/782], Loss: 1.4626\n",
      "Epoch [5/20], Step [200/782], Loss: 1.7083\n",
      "Epoch [5/20], Step [300/782], Loss: 1.5181\n",
      "Epoch [5/20], Step [400/782], Loss: 1.8916\n",
      "Epoch [5/20], Step [500/782], Loss: 1.8984\n",
      "Epoch [5/20], Step [600/782], Loss: 1.9909\n",
      "Epoch [5/20], Step [700/782], Loss: 1.9073\n",
      "Epoch 5 completed in 16.50 seconds\n",
      "Epoch [6/20], Step [100/782], Loss: 1.5413\n",
      "Epoch [6/20], Step [200/782], Loss: 1.4313\n",
      "Epoch [6/20], Step [300/782], Loss: 1.2999\n",
      "Epoch [6/20], Step [400/782], Loss: 1.8302\n",
      "Epoch [6/20], Step [500/782], Loss: 1.4567\n",
      "Epoch [6/20], Step [600/782], Loss: 1.7703\n",
      "Epoch [6/20], Step [700/782], Loss: 1.6370\n",
      "Epoch 6 completed in 16.67 seconds\n",
      "Epoch [7/20], Step [100/782], Loss: 0.7087\n",
      "Epoch [7/20], Step [200/782], Loss: 1.3655\n",
      "Epoch [7/20], Step [300/782], Loss: 1.2221\n",
      "Epoch [7/20], Step [400/782], Loss: 1.2519\n",
      "Epoch [7/20], Step [500/782], Loss: 1.4609\n",
      "Epoch [7/20], Step [600/782], Loss: 1.3996\n",
      "Epoch [7/20], Step [700/782], Loss: 1.0765\n",
      "Epoch 7 completed in 16.47 seconds\n",
      "Epoch [8/20], Step [100/782], Loss: 0.9615\n",
      "Epoch [8/20], Step [200/782], Loss: 0.8231\n",
      "Epoch [8/20], Step [300/782], Loss: 1.2772\n",
      "Epoch [8/20], Step [400/782], Loss: 1.4697\n",
      "Epoch [8/20], Step [500/782], Loss: 1.1895\n",
      "Epoch [8/20], Step [600/782], Loss: 1.2098\n",
      "Epoch [8/20], Step [700/782], Loss: 1.2000\n",
      "Epoch 8 completed in 16.71 seconds\n",
      "Epoch [9/20], Step [100/782], Loss: 0.5313\n",
      "Epoch [9/20], Step [200/782], Loss: 0.9529\n",
      "Epoch [9/20], Step [300/782], Loss: 0.9799\n",
      "Epoch [9/20], Step [400/782], Loss: 0.9840\n",
      "Epoch [9/20], Step [500/782], Loss: 0.6620\n",
      "Epoch [9/20], Step [600/782], Loss: 0.5292\n",
      "Epoch [9/20], Step [700/782], Loss: 0.6217\n",
      "Epoch 9 completed in 16.74 seconds\n",
      "Epoch [10/20], Step [100/782], Loss: 0.5163\n",
      "Epoch [10/20], Step [200/782], Loss: 0.3749\n",
      "Epoch [10/20], Step [300/782], Loss: 0.4647\n",
      "Epoch [10/20], Step [400/782], Loss: 0.6267\n",
      "Epoch [10/20], Step [500/782], Loss: 0.5026\n",
      "Epoch [10/20], Step [600/782], Loss: 0.6433\n",
      "Epoch [10/20], Step [700/782], Loss: 0.8968\n",
      "Epoch 10 completed in 16.53 seconds\n",
      "Epoch [11/20], Step [100/782], Loss: 0.2903\n",
      "Epoch [11/20], Step [200/782], Loss: 0.4643\n",
      "Epoch [11/20], Step [300/782], Loss: 0.4304\n",
      "Epoch [11/20], Step [400/782], Loss: 0.6603\n",
      "Epoch [11/20], Step [500/782], Loss: 0.4616\n",
      "Epoch [11/20], Step [600/782], Loss: 0.6838\n",
      "Epoch [11/20], Step [700/782], Loss: 0.4473\n",
      "Epoch 11 completed in 16.72 seconds\n",
      "Epoch [12/20], Step [100/782], Loss: 0.2703\n",
      "Epoch [12/20], Step [200/782], Loss: 0.1831\n",
      "Epoch [12/20], Step [300/782], Loss: 0.2862\n",
      "Epoch [12/20], Step [400/782], Loss: 0.4230\n",
      "Epoch [12/20], Step [500/782], Loss: 0.3492\n",
      "Epoch [12/20], Step [600/782], Loss: 0.5174\n",
      "Epoch [12/20], Step [700/782], Loss: 0.5218\n",
      "Epoch 12 completed in 16.31 seconds\n",
      "Epoch [13/20], Step [100/782], Loss: 0.1737\n",
      "Epoch [13/20], Step [200/782], Loss: 0.1704\n",
      "Epoch [13/20], Step [300/782], Loss: 0.2929\n",
      "Epoch [13/20], Step [400/782], Loss: 0.2181\n",
      "Epoch [13/20], Step [500/782], Loss: 0.3521\n",
      "Epoch [13/20], Step [600/782], Loss: 0.4426\n",
      "Epoch [13/20], Step [700/782], Loss: 0.7448\n",
      "Epoch 13 completed in 16.61 seconds\n",
      "Epoch [14/20], Step [100/782], Loss: 0.3731\n",
      "Epoch [14/20], Step [200/782], Loss: 0.3022\n",
      "Epoch [14/20], Step [300/782], Loss: 0.2816\n",
      "Epoch [14/20], Step [400/782], Loss: 0.3745\n",
      "Epoch [14/20], Step [500/782], Loss: 0.1245\n",
      "Epoch [14/20], Step [600/782], Loss: 0.5722\n",
      "Epoch [14/20], Step [700/782], Loss: 0.2917\n",
      "Epoch 14 completed in 16.62 seconds\n",
      "Epoch [15/20], Step [100/782], Loss: 0.2227\n",
      "Epoch [15/20], Step [200/782], Loss: 0.1431\n",
      "Epoch [15/20], Step [300/782], Loss: 0.2088\n",
      "Epoch [15/20], Step [400/782], Loss: 0.0729\n",
      "Epoch [15/20], Step [500/782], Loss: 0.3368\n",
      "Epoch [15/20], Step [600/782], Loss: 0.2223\n",
      "Epoch [15/20], Step [700/782], Loss: 0.2212\n",
      "Epoch 15 completed in 16.33 seconds\n",
      "Epoch [16/20], Step [100/782], Loss: 0.3107\n",
      "Epoch [16/20], Step [200/782], Loss: 0.1704\n",
      "Epoch [16/20], Step [300/782], Loss: 0.1046\n",
      "Epoch [16/20], Step [400/782], Loss: 0.1855\n",
      "Epoch [16/20], Step [500/782], Loss: 0.1749\n",
      "Epoch [16/20], Step [600/782], Loss: 0.2770\n",
      "Epoch [16/20], Step [700/782], Loss: 0.3227\n",
      "Epoch 16 completed in 16.67 seconds\n",
      "Epoch [17/20], Step [100/782], Loss: 0.1667\n",
      "Epoch [17/20], Step [200/782], Loss: 0.1186\n",
      "Epoch [17/20], Step [300/782], Loss: 0.1225\n",
      "Epoch [17/20], Step [400/782], Loss: 0.1621\n",
      "Epoch [17/20], Step [500/782], Loss: 0.2179\n",
      "Epoch [17/20], Step [600/782], Loss: 0.2316\n",
      "Epoch [17/20], Step [700/782], Loss: 0.1833\n",
      "Epoch 17 completed in 16.32 seconds\n",
      "Epoch [18/20], Step [100/782], Loss: 0.1275\n",
      "Epoch [18/20], Step [200/782], Loss: 0.0578\n",
      "Epoch [18/20], Step [300/782], Loss: 0.1469\n",
      "Epoch [18/20], Step [400/782], Loss: 0.0727\n",
      "Epoch [18/20], Step [500/782], Loss: 0.2996\n",
      "Epoch [18/20], Step [600/782], Loss: 0.3378\n",
      "Epoch [18/20], Step [700/782], Loss: 0.2401\n",
      "Epoch 18 completed in 16.77 seconds\n",
      "Epoch [19/20], Step [100/782], Loss: 0.2725\n",
      "Epoch [19/20], Step [200/782], Loss: 0.1882\n",
      "Epoch [19/20], Step [300/782], Loss: 0.1462\n",
      "Epoch [19/20], Step [400/782], Loss: 0.1413\n",
      "Epoch [19/20], Step [500/782], Loss: 0.0889\n",
      "Epoch [19/20], Step [600/782], Loss: 0.1801\n",
      "Epoch [19/20], Step [700/782], Loss: 0.2038\n",
      "Epoch 19 completed in 16.76 seconds\n",
      "Epoch [20/20], Step [100/782], Loss: 0.2083\n",
      "Epoch [20/20], Step [200/782], Loss: 0.0734\n",
      "Epoch [20/20], Step [300/782], Loss: 0.1569\n",
      "Epoch [20/20], Step [400/782], Loss: 0.1757\n",
      "Epoch [20/20], Step [500/782], Loss: 0.0486\n",
      "Epoch [20/20], Step [600/782], Loss: 0.1055\n",
      "Epoch [20/20], Step [700/782], Loss: 0.1658\n",
      "Epoch 20 completed in 17.06 seconds\n",
      "Test Accuracy: 45.00%\n",
      "\n",
      "Results Summary:\n",
      "========================================================================================================================\n",
      "Model          Patch   Embed   Depth   Heads   MLP     Params         FLOPs          Time/Epoch     Accuracy  \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "ViT-Tiny       4       256     4       4       2       2.16M     4.43G     18.82s       29.16%\n",
      "ViT-Small      8       256     8       4       2       4.30M     8.80G     23.10s       8.85%\n",
      "ViT-Medium     4       512     4       8       4       12.72M     26.05G     22.76s       12.60%\n",
      "ViT-Large      8       512     8       8       4       25.38M     51.98G     23.51s       8.39%\n",
      "ResNet-18      N/A     N/A     18      N/A     N/A     11.23M     22.99G     16.58s       45.00%\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "'''Problem 1\n",
    "'''\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "num_classes = 100\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "\n",
    "# Data loading and preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))  # CIFAR-100 stats\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR100(\n",
    "    root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.CIFAR100(\n",
    "    root='./data', train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Vision Transformer (ViT) implementation\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=32, patch_size=4, in_channels=3, embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size\n",
    "        )\n",
    "        \n",
    "        # Learnable position embeddings\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.n_patches + 1, embed_dim))\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        \n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        assert H == W == self.img_size, f\"Input image size ({H}*{W}) doesn't match model ({self.img_size}*{self.img_size})\"\n",
    "        \n",
    "        # Create patches\n",
    "        x = self.proj(x)  # (B, embed_dim, n_patches_h, n_patches_w)\n",
    "        x = x.flatten(2)  # (B, embed_dim, n_patches)\n",
    "        x = x.transpose(1, 2)  # (B, n_patches, embed_dim)\n",
    "        \n",
    "        # Add class token\n",
    "        cls_token = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        \n",
    "        # Add position embeddings\n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        \n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features * 4\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4., drop=0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = MLP(in_features=embed_dim, hidden_features=int(embed_dim * mlp_ratio))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=32, patch_size=4, in_channels=3, num_classes=100,\n",
    "                 embed_dim=256, depth=4, num_heads=4, mlp_ratio=4.):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_ratio)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.norm(x)\n",
    "        x = x[:, 0]  # Class token\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "# Training function\n",
    "def train_model(model, criterion, optimizer, num_epochs=20):\n",
    "    model.train()\n",
    "    total_step = len(train_loader)\n",
    "    train_times = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (i+1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        train_times.append(epoch_time)\n",
    "        print(f'Epoch {epoch+1} completed in {epoch_time:.2f} seconds')\n",
    "    \n",
    "    return train_times\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "        return accuracy\n",
    "\n",
    "# Model configurations to test\n",
    "configs = [\n",
    "    {'name': 'ViT-Tiny', 'patch_size': 4, 'embed_dim': 256, 'depth': 4, 'num_heads': 4, 'mlp_ratio': 2},\n",
    "    {'name': 'ViT-Small', 'patch_size': 8, 'embed_dim': 256, 'depth': 8, 'num_heads': 4, 'mlp_ratio': 2},\n",
    "    {'name': 'ViT-Medium', 'patch_size': 4, 'embed_dim': 512, 'depth': 4, 'num_heads': 8, 'mlp_ratio': 4},\n",
    "    {'name': 'ViT-Large', 'patch_size': 8, 'embed_dim': 512, 'depth': 8, 'num_heads': 8, 'mlp_ratio': 4},\n",
    "]\n",
    "results = []\n",
    "\n",
    "for config in configs:\n",
    "    print(f\"\\nTraining {config['name']} configuration...\")\n",
    "    print(f\"Patch size: {config['patch_size']}, Embed dim: {config['embed_dim']}, \"\n",
    "          f\"Depth: {config['depth']}, Heads: {config['num_heads']}, MLP ratio: {config['mlp_ratio']}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = VisionTransformer(\n",
    "        img_size=32,\n",
    "        patch_size=config['patch_size'],\n",
    "        embed_dim=config['embed_dim'],\n",
    "        depth=config['depth'],\n",
    "        num_heads=config['num_heads'],\n",
    "        mlp_ratio=config['mlp_ratio'],\n",
    "        num_classes=num_classes\n",
    "    ).to(device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Print model summary\n",
    "    print(\"\\nModel Summary:\")\n",
    "    summary(model, input_size=(batch_size, 3, 32, 32))\n",
    "    \n",
    "    # Train and evaluate\n",
    "    train_times = train_model(model, criterion, optimizer, num_epochs)\n",
    "    accuracy = evaluate_model(model)\n",
    "    \n",
    "    # Calculate parameters and FLOPs\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    flops = sum(p.numel() for p in model.parameters() if p.requires_grad) * 2 * 32 * 32  # Approximate\n",
    "    \n",
    "    results.append({\n",
    "        'name': config['name'],\n",
    "        'patch_size': config['patch_size'],\n",
    "        'embed_dim': config['embed_dim'],\n",
    "        'depth': config['depth'],\n",
    "        'num_heads': config['num_heads'],\n",
    "        'mlp_ratio': config['mlp_ratio'],\n",
    "        'params': total_params,\n",
    "        'flops': flops,\n",
    "        'avg_train_time': sum(train_times)/len(train_times),\n",
    "        'accuracy': accuracy\n",
    "    })\n",
    "\n",
    "# ResNet-18 baseline\n",
    "print(\"\\nTraining ResNet-18 baseline...\")\n",
    "resnet = torchvision.models.resnet18(num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(resnet.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\"\\nResNet-18 Summary:\")\n",
    "summary(resnet, input_size=(batch_size, 3, 32, 32))\n",
    "\n",
    "resnet_train_times = train_model(resnet, criterion, optimizer, num_epochs)\n",
    "resnet_accuracy = evaluate_model(resnet)\n",
    "\n",
    "resnet_params = sum(p.numel() for p in resnet.parameters())\n",
    "resnet_flops = sum(p.numel() for p in resnet.parameters() if p.requires_grad) * 2 * 32 * 32  # Approximate\n",
    "\n",
    "results.append({\n",
    "    'name': 'ResNet-18',\n",
    "    'patch_size': 'N/A',\n",
    "    'embed_dim': 'N/A',\n",
    "    'depth': 18,\n",
    "    'num_heads': 'N/A',\n",
    "    'mlp_ratio': 'N/A',\n",
    "    'params': resnet_params,\n",
    "    'flops': resnet_flops,\n",
    "    'avg_train_time': sum(resnet_train_times)/len(resnet_train_times),\n",
    "    'accuracy': resnet_accuracy\n",
    "})\n",
    "\n",
    "# Print results table\n",
    "print(\"\\nResults Summary:\")\n",
    "print(\"=\"*120)\n",
    "print(f\"{'Model':<15}{'Patch':<8}{'Embed':<8}{'Depth':<8}{'Heads':<8}{'MLP':<8}{'Params':<15}{'FLOPs':<15}{'Time/Epoch':<15}{'Accuracy':<10}\")\n",
    "print(\"-\"*120)\n",
    "for r in results:\n",
    "    print(f\"{r['name']:<15}{r['patch_size']:<8}{r['embed_dim']:<8}{r['depth']:<8}{r['num_heads']:<8}\"\n",
    "          f\"{r['mlp_ratio']:<8}{r['params']/1e6:.2f}M{'':<5}{r['flops']/1e9:.2f}G{'':<5}\"\n",
    "          f\"{r['avg_train_time']:.2f}s{'':<7}{r['accuracy']:.2f}%\")\n",
    "print(\"=\"*120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing tiny model ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([100]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([100, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch [1/5]: 100%|| 1563/1563 [01:57<00:00, 13.31it/s, loss=3.37]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 time: 117.42s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/5]: 100%|| 1563/1563 [01:53<00:00, 13.81it/s, loss=2.56]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 time: 113.22s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/5]: 100%|| 1563/1563 [01:52<00:00, 13.84it/s, loss=2.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 time: 112.93s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/5]: 100%|| 1563/1563 [01:52<00:00, 13.90it/s, loss=1.73]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 time: 112.42s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/5]: 100%|| 1563/1563 [01:54<00:00, 13.62it/s, loss=1.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 time: 114.79s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|| 313/313 [00:23<00:00, 13.53it/s]\n",
      "c:\\Users\\legon\\miniconda3\\envs\\RealTimeenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\legon\\.cache\\huggingface\\hub\\models--microsoft--swin-small-patch4-window7-224. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiny Test Accuracy: 66.60%\n",
      "tiny Avg Epoch Time: 114.15s\n",
      "\n",
      "=== Processing small model ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-small-patch4-window7-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([100, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([100]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch [1/5]: 100%|| 1563/1563 [02:42<00:00,  9.63it/s, loss=3.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 time: 162.35s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/5]: 100%|| 1563/1563 [02:41<00:00,  9.69it/s, loss=2.73]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 time: 161.34s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/5]: 100%|| 1563/1563 [02:36<00:00,  9.97it/s, loss=1.98]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 time: 156.80s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/5]: 100%|| 1563/1563 [02:34<00:00, 10.11it/s, loss=1.7] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 time: 154.65s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/5]: 100%|| 1563/1563 [02:33<00:00, 10.21it/s, loss=1.35] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 time: 153.11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|| 313/313 [00:30<00:00, 10.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small Test Accuracy: 70.36%\n",
      "small Avg Epoch Time: 157.65s\n",
      "\n",
      "=== Processing scratch model ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/5]: 100%|| 1563/1563 [04:04<00:00,  6.38it/s, loss=3.33]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 time: 244.95s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/5]: 100%|| 1563/1563 [04:04<00:00,  6.38it/s, loss=3.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 time: 244.99s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/5]: 100%|| 1563/1563 [04:05<00:00,  6.35it/s, loss=2.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 time: 246.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/5]: 100%|| 1563/1563 [04:01<00:00,  6.48it/s, loss=2.54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 time: 241.24s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/5]: 100%|| 1563/1563 [03:57<00:00,  6.58it/s, loss=2.07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 time: 237.52s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|| 313/313 [00:20<00:00, 14.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scratch Test Accuracy: 37.23%\n",
      "scratch Avg Epoch Time: 242.94s\n",
      "\n",
      "=== Results ===\n",
      "Model      | Accuracy (%) | Avg Epoch Time (s)\n",
      "----------------------------------------\n",
      "tiny       | 66.60        | 114.15            \n",
      "small      | 70.36        | 157.65            \n",
      "scratch    | 37.23        | 242.94            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''Problem 2 Fine-tuning pretrained Swin Transformer models from Hugging Face Transformers library on CIFAR-100 on\n",
    "Tiny (microsoft/swin-tiny-patch4-window7-224) and Small (microsoft/swin-small-patch4-window7-224) variants - on CIFAR-100\n",
    "'''\n",
    "# Hyperparameters\n",
    "num_epochs = 5\n",
    "batch_size = 32\n",
    "learning_rate = 2e-5\n",
    "image_size = 224  # Swin expects 224x224 input\n",
    "\n",
    "# Models to compare\n",
    "model_variants = {\n",
    "    'tiny': 'microsoft/swin-tiny-patch4-window7-224',\n",
    "    'small': 'microsoft/swin-small-patch4-window7-224',\n",
    "    'scratch': None\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Data preparation\n",
    "processor = AutoImageProcessor.from_pretrained(model_variants['tiny'])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=processor.image_mean, std=processor.image_std)\n",
    "])\n",
    "\n",
    "# CIFAR-100 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
    "                                           download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
    "                                          download=True, transform=transform)\n",
    "\n",
    "for model_name, model_path in model_variants.items():\n",
    "    print(f\"\\n=== Processing {model_name} model ===\")\n",
    "    \n",
    "    # Scratch model\n",
    "    if model_name == 'scratch':\n",
    "        config = SwinConfig(\n",
    "            image_size=image_size,\n",
    "            patch_size=4,\n",
    "            num_channels=3,\n",
    "            embed_dim=96,\n",
    "            depths=[2, 2, 6, 2],\n",
    "            num_heads=[3, 6, 12, 24],\n",
    "            window_size=7,\n",
    "            num_labels=100\n",
    "        )\n",
    "        model = SwinForImageClassification(config).to(device)\n",
    "    else:\n",
    "        model = SwinForImageClassification.from_pretrained(\n",
    "            model_path,\n",
    "            num_labels=100,  # CIFAR-100 has 100 classes\n",
    "            ignore_mismatched_sizes=True\n",
    "        ).to(device)\n",
    "        \n",
    "        # Freeze backbone parameters for pretrained models\n",
    "        for param in model.swin.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # Only train classifier head for pretrained models, all params for scratch\n",
    "    trainable_params = []\n",
    "    if model_name == 'scratch':\n",
    "        trainable_params = model.parameters()\n",
    "    else:\n",
    "        trainable_params = model.classifier.parameters()\n",
    "        for param in model.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    " \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(trainable_params, lr=learning_rate)\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    " \n",
    "    model.train()\n",
    "    epoch_times = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "        \n",
    "        for images, labels in progress_bar:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        epoch_times.append(epoch_time)\n",
    "        print(f\"Epoch {epoch+1} time: {epoch_time:.2f}s\")\n",
    "\n",
    "    # Testing\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc='Testing'):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images).logits\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    avg_epoch_time = sum(epoch_times) / len(epoch_times)\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'avg_epoch_time': avg_epoch_time\n",
    "    }\n",
    "    \n",
    "    print(f\"{model_name} Test Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"{model_name} Avg Epoch Time: {avg_epoch_time:.2f}s\")\n",
    "\n",
    "# Print results table\n",
    "print(\"\\n=== Results ===\")\n",
    "print(f\"{'Model':<10} | {'Accuracy (%)':<12} | {'Avg Epoch Time (s)':<18}\")\n",
    "print(\"-\" * 40)    \n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"{model_name:<10} | {metrics['accuracy']:<12.2f} | {metrics['avg_epoch_time']:<18.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RealTimeenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
